{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4835,"status":"ok","timestamp":1702349942175,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"0fN3AwD5ZqwW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n","from sklearn.metrics import classification_report, f1_score\n","from sklearn.pipeline import FeatureUnion, Pipeline\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction import DictVectorizer\n","from nltk import pos_tag\n","# %pip install textblob\n","from textblob import TextBlob\n","from collections import Counter\n","import re\n","from scipy.sparse import hstack\n","from sklearn.feature_selection import chi2, SelectKBest\n","from nltk import word_tokenize\n","from gensim.models import Word2Vec\n","import warnings, time\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1551,"status":"ok","timestamp":1702349943724,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"MGxY67KAZqwX","outputId":"fb970b37-2e22-4b5c-fc46-e751fac460ec"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16623,"status":"ok","timestamp":1702349960344,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"R0eT0QsFakyN","outputId":"86f2ef86-b1c0-497c-a817-afb1e4234078"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","GOOGLE_PATH_PREFIX = \"drive/MyDrive/EECS595/project/\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1702349960484,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"rSW8T90kZqwY"},"outputs":[],"source":["def load_data(path):\n","    df = pd.read_csv(path, sep='\\t', encoding='ISO-8859-1')\n","    return df"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1650,"status":"ok","timestamp":1702349962132,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"k8XpsTupZqwY"},"outputs":[],"source":["\n","train = pd.read_csv(GOOGLE_PATH_PREFIX + 'dataset/semeval2016-task6-trainingdata.txt', sep='\\t', encoding='ISO-8859-1')\n","test = pd.read_csv(GOOGLE_PATH_PREFIX + 'dataset/semeval2016-task6-testdata-gold/SemEval2016-Task6-subtaskA-testdata-gold.txt', sep='\\t', encoding='ISO-8859-1')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702349962133,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"jT0NFhElsukD"},"outputs":[],"source":["feminist = train[train[\"Target\"] == 'Feminist Movement'][['Tweet', 'Target']]\n","hillary = train[train[\"Target\"] == 'Hillary Clinton'][['Tweet', 'Target']]\n","abortion = train[train[\"Target\"] == 'Legalization of Abortion'][['Tweet', 'Target']]\n","atheism = train[train[\"Target\"] == 'Atheism'][['Tweet', 'Target']]\n","climate = train[train[\"Target\"] == 'Climate Change is a Real Concern'][['Tweet', 'Target']]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":880,"status":"ok","timestamp":1702349963011,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"lgjXqW8y4Pmj"},"outputs":[],"source":["stance_detail = pd.read_csv(GOOGLE_PATH_PREFIX + '/dataset/StanceDataset/train.csv', encoding = 'ISO-8859-1', engine='python')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702349963011,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"tCC2dKeK4mPc","outputId":"da2dfa3f-be59-4348-8dae-939e8b940e10"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['1.  The tweet explicitly expresses opinion about the target, a part of the target, or an aspect of the target.',\n","       '3.  The tweet is not explicitly expressing opinion. (For example, the tweet is simply giving information.)',\n","       '2. The tweet does NOT expresses opinion about the target but it HAS opinion about something or someone other than the target.'],\n","      dtype=object)"]},"metadata":{},"execution_count":8}],"source":["stance_detail['Opinion Towards'].unique()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963011,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"DTI5x_u1ZqwY"},"outputs":[],"source":["def split_data(train, test, name):\n","    X_train = train[train['Target']==name][['Tweet', 'Target']]\n","    y_train = train[train['Target']==name]['Stance']\n","    X_test = test[test['Target']==name][['Tweet', 'Target']]\n","    y_test = test[test['Target']==name]['Stance']\n","    return X_train, y_train, X_test, y_test"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963011,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"KQYCZpgkZqwY"},"outputs":[],"source":["def report_score(feature_union, pipeline, X_test, y_test):\n","    # X_test = feature_union.transform(X_test)\n","    prediction = pipeline.predict(X_test)\n","    report = classification_report(y_test, prediction, output_dict=True, zero_division=0)\n","    # print(classification_report(y_test, prediction, zero_division=0))\n","    f1_favor = report['FAVOR']['f1-score']\n","    f1_against = report['AGAINST']['f1-score']\n","    score = (f1_favor + f1_against)/2\n","    # print(\"The score of this model is {}.\".format(score))\n","\n","    return score"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702349963280,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"sOYU4UqdZqwY"},"outputs":[],"source":["class TextPreprocessor(BaseEstimator, TransformerMixin):\n","    def __init__(self, lower=True, remove_at=True, lemmatize=False, remove_semst=True):\n","        self.lower = lower\n","        self.remove_at = remove_at\n","        self.lemmatize = lemmatize\n","        self.lemmatizer = WordNetLemmatizer()\n","        self.remove_semst = remove_semst\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, text, y=None):\n","        processed_texts = []\n","        # for text in X:\n","        if self.lower:\n","            text = text.lower()\n","        if self.remove_at:\n","            text = re.sub(r'(@\\w+\\s?)', '', text)\n","        if self.remove_semst:\n","            text = re.sub(r'(\\#semst\\s?)', '', text)\n","        if self.lemmatize:\n","            text = ' '.join([self.lemmatizer.lemmatize(word) for word in text.split()])\n","            processed_texts.append(text)\n","        return text"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702349963280,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"6VM5ozlsZqwZ"},"outputs":[],"source":["def preprocess(text):\n","    return TextPreprocessor().transform(text)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702349963281,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"syzMkZ_RZqwZ"},"outputs":[],"source":["def transform_all(data):\n","    return data.apply(preprocess)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702349963281,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"I5csBWPNZqwZ"},"outputs":[],"source":["class ModifiedTfidfVectorizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, max_feature):\n","        self.max_feature = max_feature\n","        self.tfidf = TfidfVectorizer(max_features=self.max_feature)\n","\n","    def fit(self, X, y=None):\n","        X = X['Tweet']\n","        self.tfidf.fit(X)\n","        return self\n","    def transform(self, X):\n","        X = X['Tweet']\n","        return self.tfidf.transform(X)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":258,"status":"ok","timestamp":1702349963535,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"PYQJSOi9ZqwZ"},"outputs":[],"source":["class SentimentExtractor(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        return np.array([[TextBlob(text).sentiment.polarity, TextBlob(text).sentiment.subjectivity] for text in X['Tweet']])\n"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":150,"status":"ok","timestamp":1702356962002,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"4chxOKrNZqwZ"},"outputs":[],"source":["C = [0.01, 0.1, 1, 10]\n","logistic_fit_intercept = [True, False]\n","logistic_class_weight = [None, 'balanced']\n","logistic_solver = ['lbfgs', 'liblinear', 'newton-cg']\n","n_neighbors = [3,4,5,6,7]\n","n_estimators = [10,25,50,75,100]\n","criterion =['gini', 'entropy']\n","max_depth =  [2,3,4,5,6,7,10]\n","decision_function_shape = ['ovo', 'ovr']\n","kernel = ['linear', 'rbf']\n","gamma = ['scale', 'auto', 0.1, 1, 10]\n","activation = ['relu', 'tanh']\n","hidden_layer_sizes = [(50,), (100,)]\n","alpha = [0.0001, 0.001, 0.01]\n","learning_rate = ['constant', 'adaptive']\n","classifiers = {\n","    'LogisticRegression': LogisticRegression(),\n","    'KNN': KNeighborsClassifier(),\n","    'RandomForest': RandomForestClassifier(),\n","    'SVM': SVC(probability=True),\n","    'GradientBoosting': GradientBoostingClassifier()\n","}\n","params_grid = {\n","    'LogisticRegression':{\n","        'classifier__C': C,\n","        'classifier__fit_intercept':logistic_fit_intercept,\n","        'classifier__class_weight': logistic_class_weight,\n","        'classifier__solver': logistic_solver\n","    },\n","    'KNN': {\n","        'classifier__n_neighbors':n_neighbors\n","    },\n","    'RandomForest': {\n","        'classifier__n_estimators':n_estimators,\n","        'classifier__criterion':criterion,\n","        'classifier__max_depth':max_depth\n","    },\n","    'SVM': {\n","        'classifier__class_weight': ['balanced', None],\n","        'classifier__decision_function_shape': decision_function_shape,\n","        'classifier__kernel':kernel,\n","        'classifier__gamma':gamma\n","    },\n","    'GradientBoosting': {\n","        'classifier__max_depth':max_depth,\n","        'classifier__n_estimators': n_estimators\n","    }\n","}"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702349963535,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"7mYZBHXVZqwZ"},"outputs":[],"source":["def train_word2vec(train):\n","    tweet = transform_all(train['Tweet'])\n","    tokenized_tweet = [word_tokenize(sentence) for sentence in tweet]\n","    wrod2vec_model = Word2Vec(tokenized_tweet, vector_size=200, window=5, min_count=1, workers=4)\n","    return wrod2vec_model"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963535,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"RYhzci7NZqwZ"},"outputs":[],"source":["class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n","    # Initialize with a pre-trained Word2Vec model\n","    def __init__(self, model):\n","        self.word2vec_model = model\n","        self.vector_size = model.vector_size\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return np.array([\n","            np.mean([self.word2vec_model.wv[word] for word in doc.split() if word in self.word2vec_model.wv]\n","                    or [np.random.rand(self.vector_size)], axis=0)\n","            for doc in X['Tweet']\n","        ])"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963535,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"gEXTP0dzZqwZ"},"outputs":[],"source":["class NGramVectorizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, ngram_range_word=(1, 3), binary=True, k=200, ngram_range_char=None):\n","        self.binary = binary\n","        self.k = k\n","        self.ngram_range_char = ngram_range_char\n","        self.ngram_range_word = ngram_range_word\n","        self.word_vectorizer = CountVectorizer(ngram_range=self.ngram_range_word, binary=self.binary)\n","        if self.ngram_range_char is not None:\n","            self.char_vectorizer = CountVectorizer(analyzer='char', ngram_range=self.ngram_range_char, binary=self.binary)\n","        else:\n","            self.char_vectorizer = None\n","        self.selector = SelectKBest(chi2, k=self.k)\n","        self.is_fitted = False\n","\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def fit_selector(self, X, y):\n","        word_features = self.word_vectorizer.fit_transform(X['Tweet'])\n","        if self.char_vectorizer is not None:\n","            char_features = self.char_vectorizer.fit_transform(X['Tweet'])\n","            combined_features = hstack([word_features, char_features])\n","        else:\n","            combined_features = word_features\n","        self.selector.fit(combined_features, y)\n","        self.is_fitted = True\n","\n","    def transform(self, X):\n","        if not self.is_fitted:\n","            raise RuntimeError(\"You must call fit_selector before calling transform\")\n","        word_features = self.word_vectorizer.transform(X['Tweet'])\n","        if self.char_vectorizer is not None:\n","            char_features = self.char_vectorizer.transform(X['Tweet'])\n","            combined_features = hstack([word_features, char_features])\n","            selected_features = self.selector.transform(combined_features)\n","        else:\n","            selected_features = self.selector.transform(word_features)\n","        return selected_features"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1702349963674,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"KEw6BwmiZqwa"},"outputs":[],"source":["target_words = {\n","    'Atheism': ['atheism', 'musili', 'god'],\n","    'Hillary Clinton': ['hillary', 'clinton'],\n","    'Climate Change is a Real Concern': ['climate'],\n","    'Feminist Movement': ['feminism', 'feminist', 'female', 'woman'],\n","    'Legalization of Abortion': ['abortion', 'prolife', 'youth']\n","}\n","unique_words = set()\n","for words in target_words.values():\n","    unique_words.update(words)\n","target_words = list(unique_words)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963675,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"Oo548EAEZqwa"},"outputs":[],"source":["class TargetPresence_Single(BaseEstimator, TransformerMixin):\n","    def __init__(self, target_words_dict):\n","        self.target_words_dict = target_words_dict\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        features = []\n","\n","        for _, row in X.iterrows():\n","            tweet = row['Tweet']\n","            target = row['Target']\n","            related_words = self.target_words_dict.get(target, [])\n","\n","            # Check for the presence of each related word in the tweet\n","            presence = [int(word in tweet) for word in related_words]\n","            features.append(presence)\n","\n","        return np.array(features)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963675,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"Bedc1BVCZqwa"},"outputs":[],"source":["class TargetPresence(BaseEstimator, TransformerMixin):\n","    def __init__(self, target_words_list):\n","        self.target_words_list = target_words_list\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        features = []\n","        for tweet in X['Tweet']:\n","            presence = [int(word in tweet.split()) for word in self.target_words_list]\n","            features.append(presence)\n","\n","        return np.array(features)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963675,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"5HZ4EVGgZqwb"},"outputs":[],"source":["class GloVeVectorizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, glove_path, vector_size=200):\n","        self.glove_path = glove_path\n","        self.vector_size = vector_size\n","        self.embeddings = self.load_glove_embeddings()\n","\n","    def load_glove_embeddings(self):\n","        embeddings = {}\n","        with open(self.glove_path, 'r', encoding='utf-8') as file:\n","            for line in file:\n","                values = line.split()\n","                word = values[0]\n","                vector = np.asarray(values[1:], dtype='float32')\n","                embeddings[word] = vector\n","        return embeddings\n","\n","    def document_vector(self, doc):\n","        words = doc.split()\n","        word_vectors = [self.embeddings[word] for word in words if word in self.embeddings]\n","\n","        if len(word_vectors) == 0:\n","            return np.zeros(self.vector_size)\n","\n","\n","        word_mean_vec = np.mean(word_vectors, axis=0)\n","        # if len(word_mean_vec) != self.vector_size:\n","        #     print(word_mean_vec)\n","        return word_mean_vec\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return np.array([self.document_vector(doc) for doc in X['Tweet']])"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":153,"status":"ok","timestamp":1702349963826,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"DKMSrkqLZqwb"},"outputs":[],"source":["class PosTagVectorizer(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","      self.pos_tags = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','MD','NN','NNP','NNS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WRB']\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","      transformed = []\n","      for tweet in X['Tweet']:\n","        tag_counts = {tag: 0 for tag in self.pos_tags}\n","        counts = Counter(tag for word, tag in pos_tag(word_tokenize(tweet)))\n","        for key, value in counts.items():\n","          if key in tag_counts.keys():\n","            tag_counts[key] = value\n","        transformed.append(tag_counts)\n","      return transformed"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963826,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"shlIXaN0ZqwZ"},"outputs":[],"source":["def classifier_grid(feature_union, classify_pipeline, classifiers, params_grid, X_train, y_train, X_test, y_test):\n","    best_score = 0\n","    best_classifier = None\n","    best_classifier_name = ''\n","    X_train_transformed = feature_union.fit_transform(X_train)\n","    X_test_transformed = feature_union.transform(X_test)\n","    for classifier_name, model in classifiers.items():\n","        classify_pipeline.set_params(classifier=model)\n","        grid_search = RandomizedSearchCV(classify_pipeline, param_distributions=params_grid[classifier_name], cv=5, verbose=0, random_state=595, n_jobs=-1)\n","        start = time.time()\n","        grid_search.fit(X_train_transformed, y_train)\n","        end = time.time()\n","        # print('The best parameter for {} is {}.'.format(classifier_name, grid_search.best_params_))\n","        # print(\"Grid Search for Model {} needs {} seconds.\".format(classifier_name, end-start))\n","        # print(\"The score for {} is {:.2f}.\".format(classifier_name, grid_search.best_score_))\n","        best_model = grid_search.best_estimator_\n","        score = report_score(feature_union, best_model, X_test_transformed, y_test)\n","\n","        if score > best_score:\n","            best_score = score\n","            best_classifier = best_model\n","            best_classifier_name = classifier_name\n","\n","    return feature_union, best_classifier, best_classifier_name, best_score"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1702349963826,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"ixVWUx0pZqwZ"},"outputs":[],"source":["def find_best(feature_union, pipeline, train, test, name, classifiers, params_grid):\n","    X_train, y_train, X_test, y_test = split_data(train, test, name)\n","    feature_extraction, best_classifier, classifier_name, score = classifier_grid(feature_union, pipeline, classifiers, params_grid, X_train, y_train, X_test, y_test)\n","    print(\"The best model for {} is {} with {}\".format(name, classifier_name, best_classifier.get_params()))\n","    # print(\"The avg F1-score is \", score)\n","    return feature_extraction, best_classifier, classifier_name, score"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349963973,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"Ih-LSVKRZqwb"},"outputs":[],"source":["def train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid):\n","    trained_classifiers = {}\n","    trained_feature_extraction = {}\n","    for name in train[\"Target\"].unique():\n","        feature_extraction, classifier, classifier_name, score = find_best(feature_union, classify_pipeline, train, test, name, classifiers, params_grid)\n","        print(\"The best classifier for {} is {} with the average of F1 as {}.\".format(name, classifier_name, score))\n","        trained_classifiers[name] = classifier\n","        trained_feature_extraction[name] = feature_extraction\n","    return trained_feature_extraction, trained_classifiers"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1702349964118,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"ij4-qoWHZqwc"},"outputs":[],"source":["def test_all(test_data, trained_feature_extraction, classifiers):\n","    predictions = pd.DataFrame(index=test_data.index)\n","\n","    for target in classifiers:\n","        # Select the test data for the current target\n","        target_data = test_data[test_data['Target'] == target][['Tweet', 'Target']]\n","\n","        if not target_data.empty:\n","            classifier = classifiers[target]\n","            feature_union = trained_feature_extraction[target]\n","            transformed_features = feature_union.transform(target_data)\n","            target_predictions = classifier.predict(transformed_features)\n","            predictions.loc[target_data.index, 'Prediction'] = target_predictions\n","\n","    print(classification_report(test_data['Stance'], predictions))\n","    report = classification_report(test_data['Stance'], predictions, output_dict=True, zero_division=0)\n","    f1_favor = report['FAVOR']['f1-score']\n","    f1_against = report['AGAINST']['f1-score']\n","    score = (f1_favor + f1_against)/2\n","    print(\"The average F1-score for total test dataset is \", score)\n","    return score, predictions"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349964118,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"yHdosU0uZqwc"},"outputs":[],"source":["feature_union = FeatureUnion([\n","    ('pos_tag', Pipeline([\n","            ('pos_extractor', PosTagVectorizer()),\n","            ('vectorizer', DictVectorizer())\n","        ]))\n","    # ('tfidf', ModifiedTfidfVectorizer(max_feature=500)),\n","    # ('sentiment', SentimentExtractor()),\n","    # ('bi-gram', CountVectorizer(ngram_range=(2,2))),\n","    # ('tri-gram', CountVectorizer(ngram_range=(3,3)))\n","])\n","\n","# Model Pipeline\n","classify_pipeline = Pipeline([('classifier', None)])"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349964118,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"H1XwA2_EZqwc"},"outputs":[],"source":["def train_test_single(train, test, feature_union, pipeline, classifiers, params_grid):\n","    X_train = train[['Tweet', 'Target']]\n","    y_train = train['Stance']\n","    X_test = test[['Tweet', 'Target']]\n","    y_test = test['Stance']\n","    feature_extarction, best_classifier, classifier_name, score = classifier_grid(feature_union, pipeline, classifiers, params_grid, X_train, y_train, X_test, y_test)\n","    print(\"The best model for considering all targets is {} with {}\".format(classifier_name, best_classifier.get_params()))\n","    print(\"The avg F1-score is \", score)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702349964289,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"kJMnmLxB-7PT"},"outputs":[],"source":["def test_on_opinionA(train, dataset, feature_union, classifiers):\n","  test_data = dataset[['Tweet', 'Target', 'Stance']]\n","  feature_union.fit(dataset[['Tweet', 'Target']])\n","\n","  predictions = pd.DataFrame(index=test_data.index)\n","\n","  for target, clf in classifiers.items():\n","    X = train[train['Target']==target][['Tweet', 'Target']]\n","    y = train[train['Target']==target]['Stance']\n","    X_transformed = feature_union.transform(X)\n","    clf.fit(X_transformed, y)\n","    target_data = test_data[test_data['Target'] == target][['Tweet', 'Target']]\n","    transformed_features = feature_union.transform(target_data)\n","    target_predictions = clf.predict(transformed_features)\n","    predictions.loc[target_data.index, 'Prediction'] = target_predictions\n","\n","  print(classification_report(test_data['Stance'], predictions))\n","  report = classification_report(test_data['Stance'], predictions, output_dict=True, zero_division=0)\n","  f1_favor = report['FAVOR']['f1-score']\n","  f1_against = report['AGAINST']['f1-score']\n","  score = (f1_favor + f1_against)/2\n","  print(\"The average F1-score for total test dataset is \", score)"]},{"cell_type":"code","source":["trump = load_data(GOOGLE_PATH_PREFIX + \"dataset/semeval2016-task6-testdata-gold/SemEval2016-Task6-subtaskB-testdata-gold.txt\")\n","trump[\"Tweet\"] = transform_all(trump['Tweet'])\n","glove_vectorizer = GloVeVectorizer(GOOGLE_PATH_PREFIX+\"glove.6B.200d.txt\", vector_size=200)"],"metadata":{"id":"TqGhpgJILprq","executionInfo":{"status":"ok","timestamp":1702350029125,"user_tz":300,"elapsed":64548,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"ikT8LhV2Zqwd","executionInfo":{"status":"ok","timestamp":1702350029340,"user_tz":300,"elapsed":235,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}}},"outputs":[],"source":["def predict_model_taskB(tarin, test, feature_union, classifiers):\n","\n","  feature_union.fit(train[['Tweet', 'Target']])\n","  for target, clf in classifiers.items():\n","    X = train[train['Target']==target][['Tweet', 'Target']]\n","    y = train[train['Target']==target]['Stance']\n","    X_transformed = feature_union.transform(X)\n","    clf.fit(X_transformed, y)\n","\n","  X = test[['Tweet', 'Target']]\n","  y = test['Stance']\n","  X_transformed = feature_union.transform(X)\n","\n","  probs = []\n","  for clf in classifiers.values():\n","    prob = clf.predict_proba(X_transformed)\n","    probs.append(prob)\n","\n","  class_labels = list(classifiers.values())[0].classes_\n","\n","  best_clf = None\n","  best_f1 = 0\n","  best_f1_report = None\n","  best_f1_targetmodel = None\n","  best_prediction = None\n","  i = 0\n","  for target, clf in classifiers.items():\n","    max_prob_indices = np.argmax(probs[i], axis=-1)\n","    predicted_stances = [class_labels[idx] for idx in max_prob_indices]\n","    report = classification_report(y, predicted_stances, output_dict=True)\n","    f1_favor = report['FAVOR']['f1-score']\n","    f1_against = report['AGAINST']['f1-score']\n","    score = (f1_favor + f1_against)/2\n","    i += 1\n","    if score > best_f1:\n","      best_f1 = score\n","      best_clf = clf\n","      best_f1_report = classification_report(y, predicted_stances)\n","      best_f1_targetmodel = target\n","      best_prediction = predicted_stances\n","\n","\n","  sum_probs = np.sum(probs, axis=0)\n","  max_prob_indices = np.argmax(sum_probs, axis=-1)\n","  predicted_stances = [class_labels[idx] for idx in max_prob_indices]\n","  report = classification_report(y, predicted_stances, output_dict=True)\n","  f1_favor = report['FAVOR']['f1-score']\n","  f1_against = report['AGAINST']['f1-score']\n","  score = (f1_favor + f1_against)/2\n","\n","  if score > best_f1:\n","      best_f1 = score\n","      best_clf = \"Soft Voting Classifier\"\n","      best_f1_report = classification_report(y, predicted_stances)\n","      best_f1_targetmodel = \"Five Combined\"\n","      best_prediction = predicted_stances\n","\n","\n","  print(\"The average F1-score for task B is \", best_f1)\n","  print(\"The best Classifier for task B is \", best_clf)\n","  print(\"The classification reporst is: \", best_f1_report)\n","  print(\"The most similiar target for task B is \", best_f1_targetmodel)\n","  return predicted_stances, best_f1_targetmodel"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"QIJMUuqHAejA","executionInfo":{"status":"ok","timestamp":1702350029950,"user_tz":300,"elapsed":668,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}}},"outputs":[],"source":["stance_test = pd.read_csv(GOOGLE_PATH_PREFIX + '/dataset/StanceDataset/test.csv', encoding = 'ISO-8859-1', engine='python')\n","stance_test['Tweet'] = transform_all(stance_test['Tweet'])\n","stance_test_A = stance_test[stance_test['Target']!=\"Donald Trump\"]\n","stance_test_B = stance_test[stance_test['Target']==\"Donald Trump\"]\n","opinion_to_target_A = stance_test_A[stance_test_A['Opinion Towards'].str.startswith('1')]\n","opinion_to_other_A = stance_test_A[stance_test_A['Opinion Towards'].str.startswith('2')]\n","opinion_to_target_B = stance_test_B[stance_test_B['Opinion Towards'].str.startswith('1')]\n","opinion_to_other_B = stance_test_B[stance_test_B['Opinion Towards'].str.startswith('2')]"]},{"cell_type":"code","source":["def test_on_opinionB(train, dataset, feature_union, classifiers, best_f1_targetmodel):\n","  test_data = dataset[['Tweet', 'Target', 'Stance']]\n","  feature_union.fit(dataset[['Tweet', 'Target']])\n","\n","  predictions = pd.DataFrame(index=test_data.index)\n","  if best_f1_targetmodel != \"Five Combined\":\n","    clf = classifiers[best_f1_targetmodel]\n","    target_data = test_data[['Tweet', 'Target']]\n","    transformed_features = feature_union.transform(target_data)\n","    target_predictions = clf.predict(transformed_features)\n","    predictions.loc[target_data.index, 'Prediction'] = target_predictions\n","  else:\n","    probs = []\n","    for target, clf in classifiers.items():\n","      target_data = test_data[['Tweet', 'Target']]\n","      transformed_features = feature_union.transform(target_data)\n","      prob = clf.predict_proba(transformed_features)\n","      probs.append(prob)\n","    sum_probs = np.sum(probs, axis=0)\n","    max_prob_indices = np.argmax(sum_probs, axis=-1)\n","    class_labels = list(classifiers.values())[0].classes_\n","    predicted_stances = [class_labels[idx] for idx in max_prob_indices]\n","    predictions.loc[target_data.index, 'Prediction'] = predicted_stances\n","\n","  print(classification_report(test_data['Stance'], predictions))\n","  report = classification_report(test_data['Stance'], predictions, output_dict=True, zero_division=0)\n","  f1_favor = report['FAVOR']['f1-score']\n","  f1_against = report['AGAINST']['f1-score']\n","  score = (f1_favor + f1_against)/2\n","  print(\"The average F1-score for total test dataset is \", score)"],"metadata":{"id":"kLmkMYOE-lTq","executionInfo":{"status":"ok","timestamp":1702350029995,"user_tz":300,"elapsed":122,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["train['Tweet'] = transform_all(train['Tweet'])\n","test['Tweet'] = transform_all(test['Tweet'])\n","word2vec_model = train_word2vec(train)\n","word2vec_vectorizer = Word2VecVectorizer(word2vec_model)"],"metadata":{"id":"IlN7kmyZHSFS","executionInfo":{"status":"ok","timestamp":1702350032013,"user_tz":300,"elapsed":2139,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":1805,"status":"error","timestamp":1700939487629,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"9raII9InZqwc","outputId":"92426c70-73fc-4f1a-e913-e90accc43a36"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-a5301569363e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# only pos_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrained_feature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_classifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictinos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_feature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_classifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_test_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-d8ad121d7d8a>\u001b[0m in \u001b[0;36mtrain_test_all\u001b[0;34m(train, test, feature_union, classify_pipeline, classifiers, params_grid)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrained_feature_extraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The best classifier for {} is {} with the average of F1 as {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrained_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-c5b11865ffc6>\u001b[0m in \u001b[0;36mfind_best\u001b[0;34m(feature_union, pipeline, train, test, name, classifiers, params_grid)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The best model for {} is {} with {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(\"The avg F1-score is \", score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-a051a06acadb>\u001b[0m in \u001b[0;36mclassifier_grid\u001b[0;34m(feature_union, classify_pipeline, classifiers, params_grid, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m595\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print('The best parameter for {} is {}.'.format(classifier_name, grid_search.best_params_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1292\u001b[0m             path_func(\n\u001b[1;32m   1293\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             ]\n\u001b[0;32m--> 450\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                  **options)\n\u001b[1;32m    709\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    711\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    712\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         loss, grad_pointwise = self.base_loss.loss_gradient(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mraw_prediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_prediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/_loss/loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgradient_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mloss_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m                 \u001b[0mgradient_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train['Tweet'] = transform_all(train['Tweet'])\n","test['Tweet'] = transform_all(test['Tweet'])\n","# only pos_tag\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":540272,"status":"ok","timestamp":1700788944940,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"bxgwpo_j4Fv-","outputId":"186b59c7-a118-4e66-dad0-5c3ac647990f"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5845596310125192.\n","The best model for Climate Change is a Real Concern is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=4))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=4), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 4, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Climate Change is a Real Concern is KNN with the average of F1 as 0.4310657596371881.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10, n_estimators=75))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10, n_estimators=75), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5401860606984902.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.4867208672086721.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', kernel='linear'))], 'verbose': False, 'classifier': SVC(class_weight='balanced', kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.5918169059699881.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.57      0.62       715\n","       FAVOR       0.44      0.40      0.42       304\n","        NONE       0.24      0.39      0.30       230\n","\n","    accuracy                           0.50      1249\n","   macro avg       0.45      0.46      0.45      1249\n","weighted avg       0.54      0.50      0.51      1249\n","\n","The average F1-score for total test dataset is  0.5199976583538227\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.542533839395242\n"]}],"source":["# tfidf + bigram\n","# train = load_data(args.train_path)\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535348,"status":"ok","timestamp":1700789694453,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"ppqVbS6y55mc","outputId":"8f2cb2bd-6476-49be-988b-4ecb4a3d8ee8"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5766905508284819.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.41132075471698115.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5623493975903615.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.5002579979360166.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6120448179271709.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.62      0.65       715\n","       FAVOR       0.51      0.54      0.52       304\n","        NONE       0.24      0.29      0.26       230\n","\n","    accuracy                           0.54      1249\n","   macro avg       0.48      0.48      0.48      1249\n","weighted avg       0.56      0.54      0.55      1249\n","\n","The average F1-score for total test dataset is  0.5851964591861727\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10, n_estimators=75))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10, n_estimators=75), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5241606713078456\n"]}],"source":["# tfidf + trigram\n","# train = load_data(args.train_path)\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":622379,"status":"ok","timestamp":1700790316823,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"LnFLO_Jt58gn","outputId":"19dbc17e-e7c0-42d6-afa9-e87ff5813c2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6645716346401525.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.37288135593220345.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5525655056975471.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.46470482036468674.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.6249451032059727.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.72      0.63      0.67       715\n","       FAVOR       0.52      0.47      0.49       304\n","        NONE       0.34      0.50      0.40       230\n","\n","    accuracy                           0.57      1249\n","   macro avg       0.52      0.53      0.52      1249\n","weighted avg       0.60      0.57      0.58      1249\n","\n","The average F1-score for total test dataset is  0.5818031806645162\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5670795375340029\n"]}],"source":["# tfidf + ngram\n","# train = load_data(args.train_path)\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527677,"status":"ok","timestamp":1700790844462,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"s1f-JfQK7mbo","outputId":"585ccb56-fdd8-45bb-d35d-6b89db4f5886"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5634892511221355.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.40613026819923376.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5730294396961063.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.5412624314998985.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.5964088961436442.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.69      0.64      0.66       715\n","       FAVOR       0.48      0.53      0.50       304\n","        NONE       0.27      0.29      0.28       230\n","\n","    accuracy                           0.55      1249\n","   macro avg       0.48      0.49      0.48      1249\n","weighted avg       0.56      0.55      0.55      1249\n","\n","The average F1-score for total test dataset is  0.5825769589366399\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5050666088390572\n"]}],"source":["# tfidf + sentiment\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","# transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":517148,"status":"ok","timestamp":1700791361586,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"EcZr3-wK71y7","outputId":"108d7b7b-41da-4e54-bc34-b934a0fceb29"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5805911879531511.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(criterion='entropy', max_depth=5, n_estimators=25))], 'verbose': False, 'classifier': RandomForestClassifier(criterion='entropy', max_depth=5, n_estimators=25), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.41666666666666663.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10, n_estimators=75))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10, n_estimators=75), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5376868096166342.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.4702452838046059.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', gamma=1))], 'verbose': False, 'classifier': SVC(class_weight='balanced', gamma=1), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 1, 'classifier__kernel': 'rbf', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.5992537313432836.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.61      0.65       715\n","       FAVOR       0.52      0.53      0.53       304\n","        NONE       0.23      0.30      0.26       230\n","\n","    accuracy                           0.53      1249\n","   macro avg       0.48      0.48      0.48      1249\n","weighted avg       0.56      0.53      0.55      1249\n","\n","The average F1-score for total test dataset is  0.5859477711124681\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5516816601921897\n"]}],"source":["# tfidf + target\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","target_words = {\n","            'Atheism': ['atheism', 'god'],\n","            'Hillary Clinton': ['hillary', 'clinton'],\n","            'Climate Change is a Real Concern': ['climate'],\n","            'Feminist Movement': ['feminism', 'feminist', 'female', 'woman'],\n","            'Legalization of Abortion': ['abortion', 'prolife', 'youth']\n","        }\n","unique_words = set()\n","for words in target_words.values():\n","    unique_words.update(words)\n","target_words = list(unique_words)\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":802949,"status":"ok","timestamp":1700792290736,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"AavfagB48DEL","outputId":"b41949b2-3be2-4500-86b0-aaa0b79ec6f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6065232086525462.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.4075471698113207.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=10, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=10, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.57434774676154.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.44969180565627265.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=10, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'), 'classifier__C': 10, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6037002487562189.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.69      0.68       715\n","       FAVOR       0.56      0.51      0.53       304\n","        NONE       0.22      0.23      0.22       230\n","\n","    accuracy                           0.56      1249\n","   macro avg       0.48      0.47      0.48      1249\n","weighted avg       0.56      0.56      0.56      1249\n","\n","The average F1-score for total test dataset is  0.6045965100351379\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5585078615763116\n"]}],"source":["# tfidf + pos_tag\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":672623,"status":"ok","timestamp":1700792963343,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"DtQl43DGDGVv","outputId":"0dd535b9-99f2-4645-8f88-3d36979abeb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6597859327217125.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.3655394524959742.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5372499027699754.\n","The best model for Hillary Clinton is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Hillary Clinton is SVM with the average of F1 as 0.4792481443052119.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.6219327016520895.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.71      0.63      0.67       715\n","       FAVOR       0.51      0.43      0.47       304\n","        NONE       0.33      0.51      0.40       230\n","\n","    accuracy                           0.56      1249\n","   macro avg       0.52      0.52      0.51      1249\n","weighted avg       0.59      0.56      0.57      1249\n","\n","The average F1-score for total test dataset is  0.5661832345968449\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5606288987315353\n"]}],"source":["# tfidf + ngram + sentiment\n","# train = load_data(args.train_path)\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":627798,"status":"ok","timestamp":1700793591120,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"1NoqeD2tDKZo","outputId":"59ee9795-4cd7-49f3-a002-44e3baf3a5f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.557690283187704.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', kernel='linear'))], 'verbose': False, 'classifier': SVC(class_weight='balanced', kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.43367903930131.\n","The best model for Feminist Movement is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Feminist Movement is SVM with the average of F1 as 0.5351173895742141.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.529862240095833.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.5966230936819172.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.69      0.66      0.67       715\n","       FAVOR       0.48      0.38      0.42       304\n","        NONE       0.24      0.34      0.28       230\n","\n","    accuracy                           0.53      1249\n","   macro avg       0.47      0.46      0.46      1249\n","weighted avg       0.55      0.53      0.54      1249\n","\n","The average F1-score for total test dataset is  0.5462486310667515\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5371075471624025\n"]}],"source":["# tfidf + bigram + sentiment\n","# train = load_data(args.train_path)\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":544349,"status":"ok","timestamp":1700794135434,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"AtrdRgo4DOLj","outputId":"8a141c40-93d5-4f7c-f228-46c073d20da2"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5833333333333334.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', kernel='linear'))], 'verbose': False, 'classifier': SVC(class_weight='balanced', kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.4272445820433437.\n","The best model for Feminist Movement is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Feminist Movement is SVM with the average of F1 as 0.5527377857115138.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.47639717851329355.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6154390934844194.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.66      0.67       715\n","       FAVOR       0.49      0.31      0.38       304\n","        NONE       0.25      0.39      0.30       230\n","\n","    accuracy                           0.53      1249\n","   macro avg       0.47      0.45      0.45      1249\n","weighted avg       0.55      0.53      0.53      1249\n","\n","The average F1-score for total test dataset is  0.5228646235728389\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5445155250429085\n"]}],"source":["# tfidf + trigram + sentiment\n","# train = load_data(args.train_path)\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":841768,"status":"ok","timestamp":1700794977660,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"YVBNMADgLPjq","outputId":"5c1c817d-d12c-49a3-b7d2-67a86eb174e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5556187443130118.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.39925373134328357.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10, n_estimators=75))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10, n_estimators=75), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5416343495813695.\n","The best model for Hillary Clinton is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Hillary Clinton is SVM with the average of F1 as 0.43712472122147883.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', kernel='linear'))], 'verbose': False, 'classifier': SVC(class_weight='balanced', kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.566547192353644.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.59      0.63       715\n","       FAVOR       0.48      0.53      0.50       304\n","        NONE       0.24      0.30      0.26       230\n","\n","    accuracy                           0.52      1249\n","   macro avg       0.46      0.47      0.46      1249\n","weighted avg       0.55      0.52      0.53      1249\n","\n","The average F1-score for total test dataset is  0.5643712574850299\n","The best model for considering all targets is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5535390199637023\n"]}],"source":["# tfidf + pos_tag + bigram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":817849,"status":"ok","timestamp":1700795795923,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"S1fy4cQ5LU_l","outputId":"0becf599-88fd-4262-98b0-03ebba6654d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5636697247706423.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=7, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=7, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 7, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.4045801526717557.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5338639108313041.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 0.01, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.47697581413064183.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=10, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'), 'classifier__C': 10, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6013706975327444.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.68      0.68       715\n","       FAVOR       0.51      0.53      0.52       304\n","        NONE       0.24      0.23      0.24       230\n","\n","    accuracy                           0.56      1249\n","   macro avg       0.48      0.48      0.48      1249\n","weighted avg       0.56      0.56      0.56      1249\n","\n","The average F1-score for total test dataset is  0.5975521211492584\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5546519554223934\n"]}],"source":["# tfidf + pos_tag + trigram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":975852,"status":"ok","timestamp":1700796771758,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"BHkSlO1ULaH9","outputId":"dc5e3bb5-8154-4b75-faae-2157f9b25e1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=10, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=10, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is RandomForest with the average of F1 as 0.6986885245901638.\n","The best model for Climate Change is a Real Concern is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is GradientBoosting with the average of F1 as 0.3590909090909091.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5393433154129826.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.4857435897435897.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.6224928366762177.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.74      0.71      0.72       715\n","       FAVOR       0.54      0.45      0.49       304\n","        NONE       0.38      0.51      0.43       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.55      0.56      0.55      1249\n","weighted avg       0.62      0.61      0.61      1249\n","\n","The average F1-score for total test dataset is  0.6071171634121274\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5636149333705543\n"]}],"source":["# tfidf + pos_tag + ngram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507860,"status":"ok","timestamp":1700797279602,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"xlLT_KFoLiY3","outputId":"f18b5f77-1521-434b-9c67-4d49f72d0b76"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5635608308605341.\n","The best model for Climate Change is a Real Concern is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=4))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=4), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 4, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Climate Change is a Real Concern is KNN with the average of F1 as 0.42237947810703236.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5417682926829268.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.47912353347135955.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', kernel='linear'))], 'verbose': False, 'classifier': SVC(class_weight='balanced', kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.5971532605097649.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.63      0.65       715\n","       FAVOR       0.46      0.40      0.43       304\n","        NONE       0.26      0.36      0.30       230\n","\n","    accuracy                           0.52      1249\n","   macro avg       0.47      0.46      0.46      1249\n","weighted avg       0.55      0.52      0.53      1249\n","\n","The average F1-score for total test dataset is  0.5418764302059497\n","The best model for considering all targets is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5490721288515407\n"]}],"source":["# tfidf + target + bigram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501140,"status":"ok","timestamp":1700797780714,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"yTqeRYsRLpFA","outputId":"3ba09cf4-6562-475b-9c3d-9c6d115f9f42"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5701420969533945.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.4048582995951417.\n","The best model for Feminist Movement is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is LogisticRegression with the average of F1 as 0.5437306501547987.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.46852908610850863.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6100289127069887.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.71      0.69       715\n","       FAVOR       0.52      0.38      0.44       304\n","        NONE       0.25      0.30      0.27       230\n","\n","    accuracy                           0.55      1249\n","   macro avg       0.48      0.46      0.47      1249\n","weighted avg       0.56      0.55      0.55      1249\n","\n","The average F1-score for total test dataset is  0.5647322008532574\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5394335200755773\n"]}],"source":["# tfidf + target + trigram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":706605,"status":"ok","timestamp":1700798487294,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"ZepfE-XRLrnO","outputId":"6e8922a2-b5c0-49fc-9243-b83e47fa1d81"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6634897360703812.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.37288135593220345.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5748456790123457.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.4601697009638356.\n","The best model for Legalization of Abortion is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=7, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=7, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 7, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is RandomForest with the average of F1 as 0.6441985244802146.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.73      0.72      0.72       715\n","       FAVOR       0.56      0.46      0.50       304\n","        NONE       0.37      0.47      0.42       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.55      0.55      0.55      1249\n","weighted avg       0.62      0.61      0.61      1249\n","\n","The average F1-score for total test dataset is  0.6133280696839263\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5619178533818147\n"]}],"source":["# tfidf + target + ngram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":843449,"status":"ok","timestamp":1700799331386,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"7y4oBuX9c1WG","outputId":"07d84e1f-b3a2-4d28-acb3-535259a07e88"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.585978835978836.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=10, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'), 'classifier__C': 10, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.44056626800097637.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=5, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=5, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.5400328587075575.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 0.01, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.47223452961157886.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.6181318681318682.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.82      0.74       715\n","       FAVOR       0.57      0.33      0.42       304\n","        NONE       0.24      0.21      0.23       230\n","\n","    accuracy                           0.59      1249\n","   macro avg       0.49      0.45      0.46      1249\n","weighted avg       0.57      0.59      0.57      1249\n","\n","The average F1-score for total test dataset is  0.5778025538277904\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.501961703528834\n"]}],"source":["# tfidf + pos_tag + sentiment\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('sentiment', SentimentExtractor()))\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1024115,"status":"ok","timestamp":1700800355800,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"hykWg-HydbEJ","outputId":"aa2c1788-2b2b-48ba-ed69-06ed39cfd0dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6508683825068006.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=10, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'), 'classifier__C': 10, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.4918122270742359.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5656773750509667.\n","The best model for Hillary Clinton is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Hillary Clinton is SVM with the average of F1 as 0.5207474690888719.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6040288241074353.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.73      0.66      0.70       715\n","       FAVOR       0.52      0.47      0.49       304\n","        NONE       0.36      0.51      0.42       230\n","\n","    accuracy                           0.59      1249\n","   macro avg       0.54      0.55      0.54      1249\n","weighted avg       0.61      0.59      0.60      1249\n","\n","The average F1-score for total test dataset is  0.5927890063564457\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5659553731197046\n"]}],"source":["# tfidf + pos_tag + sentiment + ngram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('sentiment', SentimentExtractor()))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":881912,"status":"ok","timestamp":1700801237686,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"Tjt9KLf_dr3L","outputId":"84d0bc20-a756-407c-ad13-adfdb3e349f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5518123667377399.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=10, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'), 'classifier__C': 10, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.4379901960784314.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10, n_estimators=75))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10, n_estimators=75), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.536011396011396.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.5082659478885894.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', kernel='linear'))], 'verbose': False, 'classifier': SVC(class_weight='balanced', kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.5648550508958867.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.69      0.62      0.65       715\n","       FAVOR       0.45      0.42      0.43       304\n","        NONE       0.26      0.37      0.30       230\n","\n","    accuracy                           0.52      1249\n","   macro avg       0.47      0.47      0.46      1249\n","weighted avg       0.55      0.52      0.54      1249\n","\n","The average F1-score for total test dataset is  0.5439211697842743\n","The best model for considering all targets is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5531468531468531\n"]}],"source":["# tfidf + target + sentiment\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":875424,"status":"ok","timestamp":1700802113067,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"9GBaY4_VdvnP","outputId":"664958ae-933f-41ce-f2b2-a0107c780bc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.5911931818181818.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=10, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'), 'classifier__C': 10, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.43382352941176466.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5584226284273558.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.49085365853658536.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.6414984604858023.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.69      0.66      0.67       715\n","       FAVOR       0.48      0.37      0.42       304\n","        NONE       0.25      0.37      0.30       230\n","\n","    accuracy                           0.53      1249\n","   macro avg       0.47      0.46      0.46      1249\n","weighted avg       0.56      0.53      0.54      1249\n","\n","The average F1-score for total test dataset is  0.5453212958045672\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5537873510395117\n"]}],"source":["# tfidf + target + sentiment + ngram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1060642,"status":"ok","timestamp":1700842725886,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"QE-czPEkvpfo","outputId":"768da7ee-64b2-4c72-854d-e9dc066a6f3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6560134566862911.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.42307692307692313.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5294329793628532.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.5200718525460793.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6151694018902044.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.51      0.58       715\n","       FAVOR       0.52      0.52      0.52       304\n","        NONE       0.22      0.39      0.28       230\n","\n","    accuracy                           0.49      1249\n","   macro avg       0.47      0.47      0.46      1249\n","weighted avg       0.56      0.49      0.51      1249\n","\n","The average F1-score for total test dataset is  0.5517044829158608\n","The best model for considering all targets is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The avg F1-score is  0.5627080425212069\n"]}],"source":["# tfidf + target + pos_tag\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","# transformers.append(('sentiment', SentimentExtractor()))\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1010518,"status":"ok","timestamp":1700843736373,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"le4nodeCwBky","outputId":"c4ebb31f-404f-4b8d-b9a9-75b8835d097b"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6357119582926034.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', gamma=0.1))], 'verbose': False, 'classifier': SVC(class_weight='balanced', gamma=0.1), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.3946360153256705.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.53806258790436.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.4756563539997535.\n","The best model for Legalization of Abortion is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=10, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=10, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is RandomForest with the average of F1 as 0.6057692307692308.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.73      0.70      0.72       715\n","       FAVOR       0.54      0.53      0.54       304\n","        NONE       0.41      0.47      0.44       230\n","\n","    accuracy                           0.62      1249\n","   macro avg       0.56      0.57      0.56      1249\n","weighted avg       0.63      0.62      0.62      1249\n","\n","The average F1-score for total test dataset is  0.625883190883191\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.5650074703153259\n"]}],"source":["# tfidf + target + pos_tag + ngram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","# transformers.append(('sentiment', SentimentExtractor()))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":882021,"status":"ok","timestamp":1700844618367,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"qFZIAIPZwGRT","outputId":"50434530-1c7a-4ca3-c235-ceab88dc18c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6012405382674517.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.4194171495758797.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=10, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=10, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.5404002501563477.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.42889822245556136.\n","The best model for Legalization of Abortion is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is GradientBoosting with the average of F1 as 0.5046884452510587.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.64      0.76      0.69       715\n","       FAVOR       0.45      0.37      0.40       304\n","        NONE       0.32      0.21      0.25       230\n","\n","    accuracy                           0.56      1249\n","   macro avg       0.47      0.44      0.45      1249\n","weighted avg       0.53      0.56      0.54      1249\n","\n","The average F1-score for total test dataset is  0.5476555083600685\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.49337725938644494\n"]}],"source":["# tfidf + target + pos_tag + sentiment\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1057803,"status":"ok","timestamp":1700845676151,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"Dp39g09_wKBY","outputId":"17cce7d0-e595-49dc-b6b9-b8e0e49b0b43"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6493026204564667.\n","The best model for Climate Change is a Real Concern is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is GradientBoosting with the average of F1 as 0.3789954337899543.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5409312301711425.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.519241592795835.\n","The best model for Legalization of Abortion is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=7, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=7, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 7, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is RandomForest with the average of F1 as 0.6272482540673403.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.78      0.64      0.70       715\n","       FAVOR       0.55      0.49      0.52       304\n","        NONE       0.40      0.68      0.50       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.57      0.60      0.57      1249\n","weighted avg       0.65      0.61      0.62      1249\n","\n","The average F1-score for total test dataset is  0.6087983336851286\n","The best model for considering all targets is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.552444519184057\n"]}],"source":["# tfidf + target + pos_tag + sentiment + ngram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","transformers.append(('sentiment', SentimentExtractor()))\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEOmufHlEEaa"},"outputs":[],"source":["# word2vec era!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8097784,"status":"ok","timestamp":1700857235159,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"qDqwyXISF0CH","outputId":"a19a91fb-e07f-4642-d8e4-50028f6ee9fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is LogisticRegression with the average of F1 as 0.49742562929061784.\n","The best model for Climate Change is a Real Concern is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Climate Change is a Real Concern is KNN with the average of F1 as 0.3846153846153846.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=3, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=3, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 3, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.49912816041848296.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.4094690265486725.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.4427393495190105.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.87      0.75       715\n","       FAVOR       0.48      0.31      0.37       304\n","        NONE       0.29      0.16      0.20       230\n","\n","    accuracy                           0.60      1249\n","   macro avg       0.48      0.44      0.44      1249\n","weighted avg       0.55      0.60      0.56      1249\n","\n","The average F1-score for total test dataset is  0.5641719167463799\n","The best model for considering all targets is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The avg F1-score is  0.48639464424090745\n"]}],"source":["# word2vec + bigram\n","transformers = []\n","word2vec_model = train_word2vec(train)\n","word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8111996,"status":"ok","timestamp":1700865996810,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"eXJJd5woGtLz","outputId":"01f2cc17-dfe7-4194-84df-7bcde3268009"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is LogisticRegression with the average of F1 as 0.4516363636363636.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.32407407407407407.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.49071102854247595.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.4001123122279938.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.44637758743754463.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.66      0.87      0.75       715\n","       FAVOR       0.56      0.31      0.40       304\n","        NONE       0.25      0.15      0.19       230\n","\n","    accuracy                           0.60      1249\n","   macro avg       0.49      0.44      0.45      1249\n","weighted avg       0.56      0.60      0.56      1249\n","\n","The average F1-score for total test dataset is  0.575210583485336\n","The best model for considering all targets is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier())], 'verbose': False, 'classifier': KNeighborsClassifier(), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 5, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The avg F1-score is  0.4287990728668696\n"]}],"source":["# word2vec + trigram\n","transformers = []\n","word2vec_model = train_word2vec(train)\n","word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3264733,"status":"ok","timestamp":1700870558018,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"8zMtV6VxGwRW","outputId":"6654ce61-5381-4063-e2ea-0fe2e56e0cf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Atheism is SVM with the average of F1 as 0.6652046783625731.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=7, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=7, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 7, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.3317972350230415.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5382035426055969.\n","The best model for Hillary Clinton is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=4, n_estimators=75))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=4, n_estimators=75), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 4, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is RandomForest with the average of F1 as 0.44383952297621365.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6367422195377817.\n","3256.763031244278 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.71      0.76      0.73       715\n","       FAVOR       0.53      0.48      0.50       304\n","        NONE       0.36      0.33      0.34       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.53      0.52      0.52      1249\n","weighted avg       0.60      0.61      0.60      1249\n","\n","The average F1-score for total test dataset is  0.6164669616084847\n","0.8503029346466064 for test_all.\n"]}],"source":["# word2vec + ngram\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2163644,"status":"ok","timestamp":1700872721627,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"wZpXO6ofHCGM","outputId":"7a3737ce-855e-4b78-d1db-6581c4feee10"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.4436738125262716.\n","The best model for Climate Change is a Real Concern is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=5, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is GradientBoosting with the average of F1 as 0.41369047619047616.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.4854112500515868.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.462092731829574.\n","The best model for Legalization of Abortion is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is GradientBoosting with the average of F1 as 0.43601769911504423.\n","2162.2896530628204 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.90      0.77       715\n","       FAVOR       0.52      0.32      0.40       304\n","        NONE       0.23      0.10      0.13       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.47      0.44      0.43      1249\n","weighted avg       0.55      0.61      0.56      1249\n","\n","The average F1-score for total test dataset is  0.5821530430908836\n","1.3291122913360596 for test_all.\n"]}],"source":["# word2vec + sentiment\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4088153,"status":"ok","timestamp":1700943742600,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"Ph7Pac3xHFe4","outputId":"4d8e5a8d-ab7b-4cdc-bf6f-93eb9cd5196e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Atheism is KNN with the average of F1 as 0.4448484848484848.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, solver='newton-cg'), 'classifier__C': 0.01, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.39166666666666666.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.5264324324324324.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.49276171485543374.\n","The best model for Legalization of Abortion is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Legalization of Abortion is KNN with the average of F1 as 0.4424594931449938.\n","4050.015785932541 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.80      0.73       715\n","       FAVOR       0.54      0.44      0.49       304\n","        NONE       0.29      0.20      0.23       230\n","\n","    accuracy                           0.60      1249\n","   macro avg       0.50      0.48      0.48      1249\n","weighted avg       0.57      0.60      0.58      1249\n","\n","The average F1-score for total test dataset is  0.6095671506982561\n","34.65070080757141 for test_all.\n"]}],"source":["# word2vec + pos_tag\n","transformers = []\n","word2vec_model = train_word2vec(train)\n","word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2323864,"status":"ok","timestamp":1700946066440,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"mdHqC0kfHI4C","outputId":"93fed2dc-519c-43ec-b125-fc853500b0ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.44034608378870677.\n","The best model for Climate Change is a Real Concern is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is RandomForest with the average of F1 as 0.3568281938325991.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=7, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=7, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 7, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.5005760154595117.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.3989379356123465.\n","The best model for Legalization of Abortion is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is GradientBoosting with the average of F1 as 0.4052287581699347.\n","2323.402838945389 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.84      0.75       715\n","       FAVOR       0.50      0.36      0.41       304\n","        NONE       0.32      0.19      0.24       230\n","\n","    accuracy                           0.60      1249\n","   macro avg       0.50      0.46      0.47      1249\n","weighted avg       0.57      0.60      0.57      1249\n","\n","The average F1-score for total test dataset is  0.5812746611768743\n","0.370880126953125 for test_all.\n"]}],"source":["# word2vec + target\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2920537,"status":"ok","timestamp":1700951471910,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"ZIjePRKMHI-i","outputId":"7b7f28ac-5336-4bb9-eb49-4c1c0379560a"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Atheism is SVM with the average of F1 as 0.6449843260188088.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.3349056603773585.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.510556511761331.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.4696078431372549.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6035294117647059.\n","3468.4735975265503 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.75      0.69      0.72       715\n","       FAVOR       0.51      0.47      0.49       304\n","        NONE       0.37      0.51      0.43       230\n","\n","    accuracy                           0.60      1249\n","   macro avg       0.55      0.56      0.55      1249\n","weighted avg       0.62      0.60      0.61      1249\n","\n","The average F1-score for total test dataset is  0.6053998924983117\n","2.339010715484619 for test_all.\n"]}],"source":["# word2vec + sentiment + ngram\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")0.6449\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3417999,"status":"ok","timestamp":1700954889907,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"z3ZOe5fX6A0o","outputId":"07071076-b538-4172-b9ac-5329aa8b1d54"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Atheism is SVM with the average of F1 as 0.6601796407185628.\n","The best model for Climate Change is a Real Concern is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Climate Change is a Real Concern is KNN with the average of F1 as 0.3568281938325991.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5132246044835379.\n","The best model for Hillary Clinton is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', gamma=1))], 'verbose': False, 'classifier': SVC(class_weight='balanced', gamma=1), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 1, 'classifier__kernel': 'rbf', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Hillary Clinton is SVM with the average of F1 as 0.4203602848764139.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6063218390804598.\n","3410.779728412628 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.75      0.71      0.73       715\n","       FAVOR       0.52      0.49      0.50       304\n","        NONE       0.39      0.47      0.42       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.55      0.56      0.55      1249\n","weighted avg       0.63      0.61      0.62      1249\n","\n","The average F1-score for total test dataset is  0.6165342452952691\n","5.089597702026367 for test_all.\n"]}],"source":["# word2vec + target + ngram\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3742778,"status":"ok","timestamp":1700962690222,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"weAt3Vxw6FMv","outputId":"f820ca1f-528f-4dd8-8462-dab0b60d69f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6192634560906516.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', gamma=0.1))], 'verbose': False, 'classifier': SVC(class_weight='balanced', gamma=0.1), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.39062499999999994.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5399929527836506.\n","The best model for Hillary Clinton is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=5, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=5, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is RandomForest with the average of F1 as 0.4054752483243156.\n","The best model for Legalization of Abortion is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is GradientBoosting with the average of F1 as 0.607751885318296.\n","3729.4177119731903 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.71      0.76      0.73       715\n","       FAVOR       0.57      0.52      0.55       304\n","        NONE       0.38      0.36      0.37       230\n","\n","    accuracy                           0.63      1249\n","   macro avg       0.56      0.54      0.55      1249\n","weighted avg       0.62      0.63      0.62      1249\n","\n","The average F1-score for total test dataset is  0.6397946542927331\n","2.546710968017578 for test_all.\n"]}],"source":["# word2vec + pos_tag + ngram\n","transformers = []\n","word2vec_model = train_word2vec(train)\n","word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1268376,"status":"ok","timestamp":1700966232287,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"xQmX8r7e6JE9","outputId":"7692d1b9-3d60-4d1c-9eef-a3c541aa1615"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.447926267281106.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, solver='newton-cg'), 'classifier__C': 0.01, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.40239043824701193.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=4, n_estimators=75))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=4, n_estimators=75), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 4, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.4808304993735457.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.48577481840193704.\n","The best model for Legalization of Abortion is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Legalization of Abortion is KNN with the average of F1 as 0.43843466107617046.\n","3519.7807517051697 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.80      0.73       715\n","       FAVOR       0.53      0.45      0.49       304\n","        NONE       0.29      0.17      0.22       230\n","\n","    accuracy                           0.60      1249\n","   macro avg       0.50      0.48      0.48      1249\n","weighted avg       0.57      0.60      0.58      1249\n","\n","The average F1-score for total test dataset is  0.6096103554409387\n","22.28897738456726 for test_all.\n"]}],"source":["# word2vec + sentiment + POS_TAG\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3006221,"status":"ok","timestamp":1700969899651,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"QG5O85ai6P0t","outputId":"520e0cd1-8e81-4bc3-b653-fd948c749562"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6368897129052193.\n","The best model for Climate Change is a Real Concern is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is GradientBoosting with the average of F1 as 0.4343891402714932.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(criterion='entropy', max_depth=3, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(criterion='entropy', max_depth=3, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'entropy', 'classifier__max_depth': 3, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.509585798816568.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier())], 'verbose': False, 'classifier': GradientBoostingClassifier(), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.5018985676153238.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear'), 'classifier__C': 0.01, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': False, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.5650047634169577.\n","2998.4514498710632 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.71      0.72      0.71       715\n","       FAVOR       0.51      0.47      0.49       304\n","        NONE       0.41      0.42      0.41       230\n","\n","    accuracy                           0.60      1249\n","   macro avg       0.54      0.54      0.54      1249\n","weighted avg       0.60      0.60      0.60      1249\n","\n","The average F1-score for total test dataset is  0.6014266176193171\n","2.993671417236328 for test_all.\n"]}],"source":["# word2vec + sentiment + pos_tag + ngram\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1880024,"status":"ok","timestamp":1700971779653,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"6dy8v_v36Vsv","outputId":"3797dff2-45c6-49ab-9a44-6e7d60940f8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.4223433242506812.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.3480176211453745.\n","The best model for Feminist Movement is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is LogisticRegression with the average of F1 as 0.5193559438163755.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.4342425262842818.\n","The best model for Legalization of Abortion is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is GradientBoosting with the average of F1 as 0.4054875283446712.\n","1878.6693828105927 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.66      0.91      0.77       715\n","       FAVOR       0.57      0.33      0.42       304\n","        NONE       0.27      0.12      0.16       230\n","\n","    accuracy                           0.62      1249\n","   macro avg       0.50      0.45      0.45      1249\n","weighted avg       0.57      0.62      0.57      1249\n","\n","The average F1-score for total test dataset is  0.5909791224740427\n","1.3163509368896484 for test_all.\n"]}],"source":["# word2vec + sentiment + target\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2779510,"status":"ok","timestamp":1700974616054,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"},"user_tz":300},"id":"K9tVMtfx6ZQl","outputId":"399bca78-03b8-4588-caac-72c4b1845804"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Atheism is SVM with the average of F1 as 0.650611620795107.\n","The best model for Climate Change is a Real Concern is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Climate Change is a Real Concern is KNN with the average of F1 as 0.3529411764705883.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.5139007698887939.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10, n_estimators=75))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10, n_estimators=75), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 75, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.4539062180614039.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6225326839271982.\n","2772.804532766342 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.72      0.74      0.73       715\n","       FAVOR       0.55      0.47      0.51       304\n","        NONE       0.35      0.39      0.37       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.54      0.53      0.54      1249\n","weighted avg       0.61      0.61      0.61      1249\n","\n","The average F1-score for total test dataset is  0.6185663594667057\n","3.839066505432129 for test_all.\n"]}],"source":["# word2vec + sentiment + target + ngram\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"WIOPPrre6pNa","outputId":"dab7f3e5-f46f-4fd4-aa69-5bdaa2dc56ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["The best model for Atheism is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Atheism is KNN with the average of F1 as 0.46397532940846653.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, solver='newton-cg'), 'classifier__C': 0.01, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.40239043824701193.\n","The best model for Feminist Movement is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Feminist Movement is KNN with the average of F1 as 0.47117794486215536.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.47087719298245617.\n","The best model for Legalization of Abortion is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier())], 'verbose': False, 'classifier': KNeighborsClassifier(), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 5, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Legalization of Abortion is KNN with the average of F1 as 0.4645569620253164.\n","2864.509047269821 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.77      0.72       715\n","       FAVOR       0.58      0.43      0.50       304\n","        NONE       0.24      0.21      0.23       230\n","\n","    accuracy                           0.59      1249\n","   macro avg       0.50      0.47      0.48      1249\n","weighted avg       0.57      0.59      0.57      1249\n","\n","The average F1-score for total test dataset is  0.6081985786383768\n","40.26014995574951 for test_all.\n"]}],"source":["# word2vec + pos_tag + target\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYdPSQ7K6yZL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701016992532,"user_tz":300,"elapsed":3118498,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"da955a5e-2e37-48b8-c3c8-e395f08b6428"},"outputs":[{"output_type":"stream","name":"stdout","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6728571428571429.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(class_weight='balanced', gamma=0.1))], 'verbose': False, 'classifier': SVC(class_weight='balanced', gamma=0.1), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': 'balanced', 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.3929961089494164.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=4, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 4, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.5398344345712767.\n","The best model for Hillary Clinton is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 10, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is GradientBoosting with the average of F1 as 0.38864715189873417.\n","The best model for Legalization of Abortion is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is RandomForest with the average of F1 as 0.6048420615128844.\n","3105.3062074184418 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.71      0.78      0.75       715\n","       FAVOR       0.55      0.53      0.54       304\n","        NONE       0.45      0.33      0.38       230\n","\n","    accuracy                           0.64      1249\n","   macro avg       0.57      0.55      0.56      1249\n","weighted avg       0.62      0.64      0.63      1249\n","\n","The average F1-score for total test dataset is  0.6440331395109261\n","3.453730583190918 for test_all.\n"]}],"source":["# word2vec + pos_tag + target + ngram\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsrqB6mz61kx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701020042216,"user_tz":300,"elapsed":3049713,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"82479b0f-e6f1-4ca1-eead-4c7ff0a8a04b"},"outputs":[{"output_type":"stream","name":"stdout","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.47447447447447455.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, solver='newton-cg'), 'classifier__C': 0.01, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.4031620553359684.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=5, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=5, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.510176539247155.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.4872543905407755.\n","The best model for Legalization of Abortion is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Legalization of Abortion is KNN with the average of F1 as 0.4562289562289562.\n","3021.3434660434723 for train_test_all.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.80      0.74       715\n","       FAVOR       0.55      0.47      0.51       304\n","        NONE       0.30      0.19      0.23       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.51      0.49      0.49      1249\n","weighted avg       0.58      0.61      0.59      1249\n","\n","The average F1-score for total test dataset is  0.622114245297394\n","29.09402370452881 for test_all.\n"]}],"source":["# word2vec + pos_tag + target + sentiment\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_gb3UB766Yr","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1701030739022,"user_tz":300,"elapsed":15297,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"42b25b3b-8af5-474d-e214-71e8588b78e6"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-a983cccf3378>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrained_feature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_classifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for train_test_all.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-d8ad121d7d8a>\u001b[0m in \u001b[0;36mtrain_test_all\u001b[0;34m(train, test, feature_union, classify_pipeline, classifiers, params_grid)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrained_feature_extraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The best classifier for {} is {} with the average of F1 as {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrained_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-c5b11865ffc6>\u001b[0m in \u001b[0;36mfind_best\u001b[0;34m(feature_union, pipeline, train, test, name, classifiers, params_grid)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The best model for {} is {} with {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(\"The avg F1-score is \", score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-73324e733e23>\u001b[0m in \u001b[0;36mclassifier_grid\u001b[0;34m(feature_union, classify_pipeline, classifiers, params_grid, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m595\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print('The best parameter for {} is {}.'.format(classifier_name, grid_search.best_params_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# word2vec + pos_tag + target + sentiment + ngram\n","transformers = []\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","classify_pipeline = Pipeline([('classifier', None)])\n","\n","start = time.time()\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","end = time.time()\n","print(end-start, \"for train_test_all.\")\n","\n","start = time.time()\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","end = time.time()\n","print(end-start, \"for test_all.\")\n","# train_test_single(train, test, feature_union, classify_pipeline, classifiers, params_grid)"]},{"cell_type":"code","source":["## 0.6674 glove pos-tag bigram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': GradientBoostingClassifier(max_depth=4),\n","    'Climate Change is a Real Concern': GradientBoostingClassifier(n_estimators=25),\n","    'Feminist Movement': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","    'Hillary Clinton': LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,max_iter=1000, solver='liblinear'),\n","    'Legalization of Abortion': LogisticRegression(C=1, max_iter=1000)\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LkmK673lLdx6","executionInfo":{"status":"ok","timestamp":1701061255677,"user_tz":300,"elapsed":104409,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"fa904690-a7c3-4423-98e5-c3e0fa36a058"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.81      0.81      0.81       535\n","       FAVOR       0.76      0.53      0.62       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.71       824\n","   macro avg       0.52      0.45      0.48       824\n","weighted avg       0.79      0.71      0.74       824\n","\n","The average F1-score for total test dataset is  0.7166604112566897\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.53      0.76      0.63       173\n","       FAVOR       0.15      0.53      0.24        15\n","        NONE       0.70      0.29      0.41       194\n","\n","    accuracy                           0.51       382\n","   macro avg       0.46      0.53      0.42       382\n","weighted avg       0.60      0.51      0.50       382\n","\n","The average F1-score for total test dataset is  0.43044326735433514\n","The average F1-score for task B is  0.392631203777956\n","The best Classifier for task B is  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.44      0.70      0.54       299\n","       FAVOR       0.28      0.22      0.25       148\n","        NONE       0.40      0.18      0.25       260\n","\n","    accuracy                           0.41       707\n","   macro avg       0.38      0.37      0.35       707\n","weighted avg       0.39      0.41      0.37       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.59      0.68      0.63       176\n","       FAVOR       0.51      0.22      0.31       145\n","        NONE       0.04      0.67      0.07         3\n","\n","    accuracy                           0.48       324\n","   macro avg       0.38      0.52      0.34       324\n","weighted avg       0.55      0.48      0.48       324\n","\n","The average F1-score for total test dataset is  0.4688067837674137\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.35      0.72      0.47       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.67      0.15      0.25       231\n","\n","    accuracy                           0.35       356\n","   macro avg       0.34      0.29      0.24       356\n","weighted avg       0.56      0.35      0.32       356\n","\n","The average F1-score for total test dataset is  0.23342175066312998\n"]}]},{"cell_type":"code","source":["## 0.6627 glove pos-tag trigram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism':  SVC(gamma=0.1, kernel='linear', probability=True),\n","    'Climate Change is a Real Concern': GradientBoostingClassifier(max_depth=4),\n","    'Feminist Movement': GradientBoostingClassifier(n_estimators=25),\n","    'Hillary Clinton': LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,max_iter=1000, solver='liblinear'),\n","    'Legalization of Abortion': LogisticRegression(C=1, max_iter=1000)\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLOw8iS8Hnjc","executionInfo":{"status":"ok","timestamp":1701061796047,"user_tz":300,"elapsed":94111,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"38afd41a-37a9-4753-ffd6-3fbfb6e83782"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.80      0.82      0.81       535\n","       FAVOR       0.76      0.52      0.62       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.71       824\n","   macro avg       0.52      0.45      0.48       824\n","weighted avg       0.79      0.71      0.74       824\n","\n","The average F1-score for total test dataset is  0.7143169207916467\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.53      0.76      0.62       173\n","       FAVOR       0.18      0.53      0.27        15\n","        NONE       0.69      0.32      0.44       194\n","\n","    accuracy                           0.53       382\n","   macro avg       0.47      0.54      0.44       382\n","weighted avg       0.60      0.53      0.51       382\n","\n","The average F1-score for total test dataset is  0.4467571158259189\n","The average F1-score for task B is  0.39109956147474423\n","The best Classifier for task B is  GradientBoostingClassifier(n_estimators=25)\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.44      0.74      0.55       299\n","       FAVOR       0.26      0.20      0.23       148\n","        NONE       0.40      0.14      0.21       260\n","\n","    accuracy                           0.41       707\n","   macro avg       0.37      0.36      0.33       707\n","weighted avg       0.39      0.41      0.36       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.58      0.71      0.64       176\n","       FAVOR       0.46      0.20      0.28       145\n","        NONE       0.02      0.33      0.04         3\n","\n","    accuracy                           0.48       324\n","   macro avg       0.35      0.41      0.32       324\n","weighted avg       0.52      0.48      0.47       324\n","\n","The average F1-score for total test dataset is  0.4583006279434851\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.36      0.78      0.49       122\n","       FAVOR       0.02      0.33      0.04         3\n","        NONE       0.71      0.13      0.22       231\n","\n","    accuracy                           0.35       356\n","   macro avg       0.36      0.41      0.25       356\n","weighted avg       0.59      0.35      0.31       356\n","\n","The average F1-score for total test dataset is  0.2644532039619972\n"]}]},{"cell_type":"code","source":["## 0.6522 glove\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': LogisticRegression(C=1),\n","    'Climate Change is a Real Concern': RandomForestClassifier(max_depth=6),\n","    'Feminist Movement': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,solver='liblinear'),\n","    'Hillary Clinton': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'),\n","    'Legalization of Abortion': KNeighborsClassifier()\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRY_8I0D-HO4","executionInfo":{"status":"ok","timestamp":1701061633699,"user_tz":300,"elapsed":35865,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"6bafdf2d-b86f-4af8-b8c6-325f314d3fe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.84      0.77      0.80       535\n","       FAVOR       0.68      0.58      0.63       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.70       824\n","   macro avg       0.51      0.45      0.48       824\n","weighted avg       0.78      0.70      0.74       824\n","\n","The average F1-score for total test dataset is  0.7142134335556789\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.54      0.65      0.59       173\n","       FAVOR       0.12      0.53      0.20        15\n","        NONE       0.74      0.40      0.52       194\n","\n","    accuracy                           0.52       382\n","   macro avg       0.47      0.53      0.44       382\n","weighted avg       0.62      0.52      0.54       382\n","\n","The average F1-score for total test dataset is  0.3938045965896271\n","The average F1-score for task B is  0.3606060606060606\n","The best Classifier for task B is  LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear')\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      0.37      0.40       299\n","       FAVOR       0.29      0.36      0.32       148\n","        NONE       0.37      0.37      0.37       260\n","\n","    accuracy                           0.37       707\n","   macro avg       0.36      0.37      0.36       707\n","weighted avg       0.38      0.37      0.37       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.61      0.35      0.45       176\n","       FAVOR       0.53      0.36      0.43       145\n","        NONE       0.02      0.67      0.03         3\n","\n","    accuracy                           0.36       324\n","   macro avg       0.38      0.46      0.30       324\n","weighted avg       0.57      0.36      0.43       324\n","\n","The average F1-score for total test dataset is  0.4370133522811381\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.33      0.40      0.36       122\n","       FAVOR       0.01      0.33      0.02         3\n","        NONE       0.65      0.36      0.47       231\n","\n","    accuracy                           0.38       356\n","   macro avg       0.33      0.37      0.28       356\n","weighted avg       0.54      0.38      0.43       356\n","\n","The average F1-score for total test dataset is  0.1931574871304269\n"]}]},{"cell_type":"code","source":["## 0.6563 bigram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,solver='liblinear'),\n","    'Climate Change is a Real Concern': LogisticRegression(C=0.1, solver='newton-cg'),\n","    'Feminist Movement': SVC(gamma=10, probability = True),\n","    'Hillary Clinton': GradientBoostingClassifier(max_depth=6, n_estimators=25),\n","    'Legalization of Abortion': KNeighborsClassifier()\n","}\n","\n","transformers = []\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4OAgr0O3M2gl","executionInfo":{"status":"ok","timestamp":1701062357294,"user_tz":300,"elapsed":5071,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"68fc5ea6-d78a-47b6-a12e-6231cec487bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.76      0.95      0.85       535\n","       FAVOR       0.87      0.44      0.58       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.77       824\n","   macro avg       0.54      0.46      0.48       824\n","weighted avg       0.80      0.77      0.75       824\n","\n","The average F1-score for total test dataset is  0.7139106205122709\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.49      0.94      0.65       173\n","       FAVOR       0.12      0.33      0.18        15\n","        NONE       0.82      0.05      0.09       194\n","\n","    accuracy                           0.46       382\n","   macro avg       0.48      0.44      0.30       382\n","weighted avg       0.64      0.46      0.34       382\n","\n","The average F1-score for total test dataset is  0.4104284615922276\n","The average F1-score for task B is  0.39055677779658154\n","The best Classifier for task B is  Soft Voting Classifier\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.44      0.92      0.59       299\n","       FAVOR       0.43      0.12      0.19       148\n","        NONE       0.35      0.04      0.08       260\n","\n","    accuracy                           0.43       707\n","   macro avg       0.41      0.36      0.29       707\n","weighted avg       0.40      0.43      0.32       707\n","\n","The most similiar target for task B is  Five Combined\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.56      0.91      0.70       176\n","       FAVOR       0.77      0.12      0.20       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.55       324\n","   macro avg       0.45      0.34      0.30       324\n","weighted avg       0.65      0.55      0.47       324\n","\n","The average F1-score for total test dataset is  0.4496224941421505\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.35      0.94      0.51       122\n","       FAVOR       0.07      0.33      0.11         3\n","        NONE       0.85      0.05      0.09       231\n","\n","    accuracy                           0.36       356\n","   macro avg       0.42      0.44      0.24       356\n","weighted avg       0.67      0.36      0.23       356\n","\n","The average F1-score for total test dataset is  0.3111111111111111\n"]}]},{"cell_type":"code","source":["## 0.6589 trigram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'),\n","    'Climate Change is a Real Concern': LogisticRegression(C=1),\n","    'Feminist Movement': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'),\n","    'Hillary Clinton': LogisticRegression(C=0.01, class_weight='balanced', fit_intercept=False,solver='liblinear'),\n","    'Legalization of Abortion': GradientBoostingClassifier(max_depth=5, n_estimators=25)\n","}\n","\n","transformers = []\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0PvHFdtM2u-","executionInfo":{"status":"ok","timestamp":1701062381814,"user_tz":300,"elapsed":2279,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"fdceaafb-d291-4e0e-e7a9-2779abfd7889"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.76      0.98      0.85       535\n","       FAVOR       0.93      0.41      0.57       289\n","\n","    accuracy                           0.78       824\n","   macro avg       0.84      0.70      0.71       824\n","weighted avg       0.82      0.78      0.76       824\n","\n","The average F1-score for total test dataset is  0.7126665848474754\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.49      0.99      0.66       173\n","       FAVOR       0.12      0.27      0.17        15\n","        NONE       1.00      0.01      0.01       194\n","\n","    accuracy                           0.46       382\n","   macro avg       0.54      0.42      0.28       382\n","weighted avg       0.73      0.46      0.31       382\n","\n","The average F1-score for total test dataset is  0.4126925898752752\n","The average F1-score for task B is  0.36885070493454175\n","The best Classifier for task B is  Soft Voting Classifier\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      1.00      0.60       299\n","       FAVOR       0.92      0.07      0.14       148\n","        NONE       0.00      0.00      0.00       260\n","\n","    accuracy                           0.44       707\n","   macro avg       0.45      0.36      0.25       707\n","weighted avg       0.37      0.44      0.28       707\n","\n","The most similiar target for task B is  Five Combined\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.56      1.00      0.72       176\n","       FAVOR       1.00      0.07      0.13       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.57       324\n","   macro avg       0.52      0.36      0.28       324\n","weighted avg       0.75      0.57      0.45       324\n","\n","The average F1-score for total test dataset is  0.42443432944125603\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.34      0.99      0.51       122\n","       FAVOR       0.50      0.33      0.40         3\n","        NONE       0.00      0.00      0.00       231\n","\n","    accuracy                           0.34       356\n","   macro avg       0.28      0.44      0.30       356\n","weighted avg       0.12      0.34      0.18       356\n","\n","The average F1-score for total test dataset is  0.4542016806722689\n"]}]},{"cell_type":"code","source":["## 0.6537 target\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': LogisticRegression(C=0.01, solver='newton-cg'),\n","    'Climate Change is a Real Concern': LogisticRegression(C=0.01, solver='newton-cg'),\n","    'Feminist Movement': KNeighborsClassifier(n_neighbors=4),\n","    'Hillary Clinton': KNeighborsClassifier(n_neighbors=7),\n","    'Legalization of Abortion': LogisticRegression(C=0.01, solver='newton-cg')\n","}\n","\n","transformers = []\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Knr-BmVaM25o","executionInfo":{"status":"ok","timestamp":1701062662683,"user_tz":300,"elapsed":4020,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"bed43dfc-09a2-442d-c587-97397fcb1d7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.75      0.76      0.76       535\n","       FAVOR       0.88      0.42      0.57       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.64       824\n","   macro avg       0.54      0.39      0.44       824\n","weighted avg       0.80      0.64      0.69       824\n","\n","The average F1-score for total test dataset is  0.6622024489050381\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.52      0.79      0.62       173\n","       FAVOR       0.11      0.27      0.16        15\n","        NONE       0.54      0.23      0.32       194\n","\n","    accuracy                           0.48       382\n","   macro avg       0.39      0.43      0.37       382\n","weighted avg       0.51      0.48      0.45       382\n","\n","The average F1-score for total test dataset is  0.39192660550458713\n","The average F1-score for task B is  0.298\n","The best Classifier for task B is  KNeighborsClassifier(n_neighbors=4)\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      1.00      0.60       299\n","       FAVOR       0.00      0.00      0.00       148\n","        NONE       0.60      0.01      0.02       260\n","\n","    accuracy                           0.43       707\n","   macro avg       0.34      0.34      0.21       707\n","weighted avg       0.40      0.43      0.26       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.54      1.00      0.71       176\n","       FAVOR       0.00      0.00      0.00       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.54       324\n","   macro avg       0.18      0.33      0.24       324\n","weighted avg       0.30      0.54      0.38       324\n","\n","The average F1-score for total test dataset is  0.35270541082164325\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.34      0.99      0.51       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.67      0.01      0.02       231\n","\n","    accuracy                           0.35       356\n","   macro avg       0.34      0.33      0.18       356\n","weighted avg       0.55      0.35      0.19       356\n","\n","The average F1-score for total test dataset is  0.2552742616033755\n"]}]},{"cell_type":"code","source":["## 0.6551 trigram target\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'),\n","    'Climate Change is a Real Concern': KNeighborsClassifier(n_neighbors=4),\n","    'Feminist Movement': LogisticRegression(C=10, class_weight='balanced', solver='liblinear'),\n","    'Hillary Clinton': SVC(gamma=10, probability=True),\n","    'Legalization of Abortion': LogisticRegression(C=10, class_weight='balanced', fit_intercept=False, solver='newton-cg')\n","}\n","\n","transformers = []\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2RBWHuyMM3Bq","executionInfo":{"status":"ok","timestamp":1701062858048,"user_tz":300,"elapsed":4168,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"9c5874c3-4220-47c3-c484-746e817b145b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.76      0.96      0.85       535\n","       FAVOR       0.86      0.44      0.59       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.78       824\n","   macro avg       0.54      0.47      0.48       824\n","weighted avg       0.80      0.78      0.76       824\n","\n","The average F1-score for total test dataset is  0.7185260132004463\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.48      0.95      0.64       173\n","       FAVOR       0.11      0.27      0.15        15\n","        NONE       1.00      0.01      0.01       194\n","\n","    accuracy                           0.45       382\n","   macro avg       0.53      0.41      0.27       382\n","weighted avg       0.73      0.45      0.30       382\n","\n","The average F1-score for total test dataset is  0.39523913997367266\n","The average F1-score for task B is  0.33826033121807775\n","The best Classifier for task B is  LogisticRegression(C=10, class_weight='balanced', fit_intercept=False,\n","                   solver='newton-cg')\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      1.00      0.60       299\n","       FAVOR       0.75      0.04      0.08       148\n","        NONE       0.00      0.00      0.00       260\n","\n","    accuracy                           0.43       707\n","   macro avg       0.39      0.35      0.23       707\n","weighted avg       0.34      0.43      0.27       707\n","\n","The most similiar target for task B is  Legalization of Abortion\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.56      1.00      0.72       176\n","       FAVOR       1.00      0.03      0.07       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.56       324\n","   macro avg       0.52      0.34      0.26       324\n","weighted avg       0.75      0.56      0.42       324\n","\n","The average F1-score for total test dataset is  0.39178547182620505\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.34      0.99      0.51       122\n","       FAVOR       0.33      0.33      0.33         3\n","        NONE       0.00      0.00      0.00       231\n","\n","    accuracy                           0.34       356\n","   macro avg       0.23      0.44      0.28       356\n","weighted avg       0.12      0.34      0.18       356\n","\n","The average F1-score for total test dataset is  0.4214035087719298\n"]}]},{"cell_type":"code","source":["## 0.6546 glove target\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': LogisticRegression(C=1, max_iter=1000),\n","    'Climate Change is a Real Concern': GradientBoostingClassifier(max_depth=4),\n","    'Feminist Movement': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","    'Hillary Clinton': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000,solver='liblinear'),\n","    'Legalization of Abortion': SVC(gamma=0.1, kernel='linear', probability=True)\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BsKVstEqM3Iw","executionInfo":{"status":"ok","timestamp":1701063165859,"user_tz":300,"elapsed":58295,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"41263f70-49c1-47a2-9607-20f12f0ecb7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.81      0.80      0.81       535\n","       FAVOR       0.73      0.54      0.62       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.71       824\n","   macro avg       0.51      0.45      0.48       824\n","weighted avg       0.78      0.71      0.74       824\n","\n","The average F1-score for total test dataset is  0.7129168349715005\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.52      0.68      0.59       173\n","       FAVOR       0.14      0.40      0.20        15\n","        NONE       0.67      0.39      0.49       194\n","\n","    accuracy                           0.52       382\n","   macro avg       0.44      0.49      0.43       382\n","weighted avg       0.58      0.52      0.52       382\n","\n","The average F1-score for total test dataset is  0.3974342636251646\n","The average F1-score for task B is  0.4429113712063415\n","The best Classifier for task B is  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      0.85      0.57       299\n","       FAVOR       0.40      0.26      0.31       148\n","        NONE       0.41      0.05      0.08       260\n","\n","    accuracy                           0.43       707\n","   macro avg       0.41      0.38      0.32       707\n","weighted avg       0.42      0.43      0.34       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.60      0.87      0.71       176\n","       FAVOR       0.70      0.26      0.38       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.59       324\n","   macro avg       0.43      0.38      0.36       324\n","weighted avg       0.64      0.59      0.55       324\n","\n","The average F1-score for total test dataset is  0.5443035036614945\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.32      0.81      0.46       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.64      0.03      0.06       231\n","\n","    accuracy                           0.30       356\n","   macro avg       0.32      0.28      0.17       356\n","weighted avg       0.52      0.30      0.20       356\n","\n","The average F1-score for total test dataset is  0.23076923076923075\n"]}]},{"cell_type":"code","source":["## 0.6597 glove bigram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': GradientBoostingClassifier(max_depth=4),\n","    'Climate Change is a Real Concern': GradientBoostingClassifier(n_estimators=25),\n","    'Feminist Movement': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,solver='liblinear'),\n","    'Hillary Clinton': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'),\n","    'Legalization of Abortion': KNeighborsClassifier(n_neighbors=4)\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JUeT4zgM3Py","executionInfo":{"status":"ok","timestamp":1701063446030,"user_tz":300,"elapsed":169456,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"b09589ef-dbe7-4687-9fa3-bc9025b03b1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.82      0.79      0.80       535\n","       FAVOR       0.72      0.53      0.61       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.70       824\n","   macro avg       0.51      0.44      0.47       824\n","weighted avg       0.78      0.70      0.74       824\n","\n","The average F1-score for total test dataset is  0.7073091536739449\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.56      0.69      0.62       173\n","       FAVOR       0.16      0.60      0.25        15\n","        NONE       0.71      0.40      0.51       194\n","\n","    accuracy                           0.54       382\n","   macro avg       0.47      0.56      0.46       382\n","weighted avg       0.62      0.54      0.55       382\n","\n","The average F1-score for total test dataset is  0.4334832904884319\n","The average F1-score for task B is  0.35351624093345946\n","The best Classifier for task B is  KNeighborsClassifier(n_neighbors=4)\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      0.65      0.52       299\n","       FAVOR       0.23      0.16      0.19       148\n","        NONE       0.38      0.22      0.28       260\n","\n","    accuracy                           0.39       707\n","   macro avg       0.35      0.34      0.33       707\n","weighted avg       0.37      0.39      0.36       707\n","\n","The most similiar target for task B is  Legalization of Abortion\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.57      0.67      0.62       176\n","       FAVOR       0.44      0.17      0.24       145\n","        NONE       0.02      0.33      0.03         3\n","\n","    accuracy                           0.44       324\n","   macro avg       0.34      0.39      0.30       324\n","weighted avg       0.51      0.44      0.44       324\n","\n","The average F1-score for total test dataset is  0.4295035386355863\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.33      0.62      0.43       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.64      0.21      0.32       231\n","\n","    accuracy                           0.35       356\n","   macro avg       0.32      0.28      0.25       356\n","weighted avg       0.53      0.35      0.35       356\n","\n","The average F1-score for total test dataset is  0.2152974504249292\n"]}]},{"cell_type":"code","source":["## 0.6526 glove trigram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': SVC(gamma=1, kernel='linear', probability=True),\n","    'Climate Change is a Real Concern': LogisticRegression(C=1),\n","    'Feminist Movement': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,solver='liblinear'),\n","    'Hillary Clinton': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'),\n","    'Legalization of Abortion': SVC(gamma=0.1, kernel='linear', probability=True)\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0CTSOYKM3Y2","executionInfo":{"status":"ok","timestamp":1701063451451,"user_tz":300,"elapsed":5438,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"12a194b9-e95d-423b-d337-d1ca7bebedbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.82      0.76      0.79       535\n","       FAVOR       0.70      0.53      0.61       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.68       824\n","   macro avg       0.51      0.43      0.46       824\n","weighted avg       0.78      0.68      0.72       824\n","\n","The average F1-score for total test dataset is  0.697324363580766\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.54      0.60      0.57       173\n","       FAVOR       0.14      0.53      0.22        15\n","        NONE       0.65      0.45      0.53       194\n","\n","    accuracy                           0.52       382\n","   macro avg       0.44      0.53      0.44       382\n","weighted avg       0.58      0.52      0.53       382\n","\n","The average F1-score for total test dataset is  0.39407814407814407\n","The average F1-score for task B is  0.3606060606060606\n","The best Classifier for task B is  LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,\n","                   solver='liblinear')\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      0.37      0.40       299\n","       FAVOR       0.29      0.36      0.32       148\n","        NONE       0.37      0.37      0.37       260\n","\n","    accuracy                           0.37       707\n","   macro avg       0.36      0.37      0.36       707\n","weighted avg       0.38      0.37      0.37       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.61      0.35      0.45       176\n","       FAVOR       0.53      0.36      0.43       145\n","        NONE       0.02      0.67      0.03         3\n","\n","    accuracy                           0.36       324\n","   macro avg       0.38      0.46      0.30       324\n","weighted avg       0.57      0.36      0.43       324\n","\n","The average F1-score for total test dataset is  0.4370133522811381\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.33      0.40      0.36       122\n","       FAVOR       0.01      0.33      0.02         3\n","        NONE       0.65      0.36      0.47       231\n","\n","    accuracy                           0.38       356\n","   macro avg       0.33      0.37      0.28       356\n","weighted avg       0.54      0.38      0.43       356\n","\n","The average F1-score for total test dataset is  0.1931574871304269\n"]}]},{"cell_type":"code","source":["## 0.6572 glove sentiment\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': SVC(gamma=0.1, kernel='linear', probability=True),\n","    'Climate Change is a Real Concern': LogisticRegression(C=1, class_weight='balanced', fit_intercept=False,solver='liblinear'),\n","    'Feminist Movement': GradientBoostingClassifier(max_depth=4, n_estimators=10),\n","    'Hillary Clinton': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'),\n","    'Legalization of Abortion': KNeighborsClassifier()\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8Lb3_EqM5bJ","executionInfo":{"status":"ok","timestamp":1701063714254,"user_tz":300,"elapsed":30537,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"7456610c-58f5-4194-cf39-dfbb258ba7a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.81      0.83      0.82       535\n","       FAVOR       0.71      0.52      0.60       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.72       824\n","   macro avg       0.51      0.45      0.47       824\n","weighted avg       0.77      0.72      0.74       824\n","\n","The average F1-score for total test dataset is  0.7070999670320308\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.55      0.79      0.65       173\n","       FAVOR       0.13      0.40      0.20        15\n","        NONE       0.74      0.34      0.47       194\n","\n","    accuracy                           0.54       382\n","   macro avg       0.47      0.51      0.44       382\n","weighted avg       0.63      0.54      0.54       382\n","\n","The average F1-score for total test dataset is  0.423040380047506\n","The average F1-score for task B is  0.3839093110279551\n","The best Classifier for task B is  GradientBoostingClassifier(max_depth=4, n_estimators=10)\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      0.76      0.55       299\n","       FAVOR       0.24      0.20      0.22       148\n","        NONE       0.28      0.06      0.10       260\n","\n","    accuracy                           0.38       707\n","   macro avg       0.32      0.34      0.29       707\n","weighted avg       0.34      0.38      0.31       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.56      0.73      0.63       176\n","       FAVOR       0.47      0.21      0.29       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.49       324\n","   macro avg       0.34      0.31      0.31       324\n","weighted avg       0.51      0.49      0.47       324\n","\n","The average F1-score for total test dataset is  0.46049398680977627\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.35      0.80      0.49       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.57      0.05      0.10       231\n","\n","    accuracy                           0.31       356\n","   macro avg       0.31      0.28      0.19       356\n","weighted avg       0.49      0.31      0.23       356\n","\n","The average F1-score for total test dataset is  0.2443324937027708\n"]}]},{"cell_type":"code","source":["## 0.6536 glove target trigram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# train['Tweet'] = transform_all(train['Tweet'])\n","# X_train = train[['Tweet', 'Target']]\n","# y_train = train['Stance']\n","\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': SVC(gamma=0.1, kernel='linear', probability=True),\n","    'Climate Change is a Real Concern': LogisticRegression(C=1, max_iter=1000),\n","    'Feminist Movement': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","    'Hillary Clinton': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000,solver='liblinear'),\n","    'Legalization of Abortion': SVC(gamma=0.1, kernel='linear', probability=True)\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","trigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lu6Tzyl-M5gh","executionInfo":{"status":"ok","timestamp":1701063870873,"user_tz":300,"elapsed":11767,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"3eab4136-27d0-434b-f4f8-ad316e3f40e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.80      0.79      0.80       535\n","       FAVOR       0.71      0.51      0.59       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.69       824\n","   macro avg       0.51      0.43      0.46       824\n","weighted avg       0.77      0.69      0.73       824\n","\n","The average F1-score for total test dataset is  0.696182820987467\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.51      0.66      0.58       173\n","       FAVOR       0.11      0.27      0.15        15\n","        NONE       0.64      0.39      0.49       194\n","\n","    accuracy                           0.51       382\n","   macro avg       0.42      0.44      0.40       382\n","weighted avg       0.56      0.51      0.51       382\n","\n","The average F1-score for total test dataset is  0.36441642173129796\n","The average F1-score for task B is  0.4429113712063415\n","The best Classifier for task B is  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      0.85      0.57       299\n","       FAVOR       0.40      0.26      0.31       148\n","        NONE       0.41      0.05      0.08       260\n","\n","    accuracy                           0.43       707\n","   macro avg       0.41      0.38      0.32       707\n","weighted avg       0.42      0.43      0.34       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.60      0.87      0.71       176\n","       FAVOR       0.70      0.26      0.38       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.59       324\n","   macro avg       0.43      0.38      0.36       324\n","weighted avg       0.64      0.59      0.55       324\n","\n","The average F1-score for total test dataset is  0.5443035036614945\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.32      0.81      0.46       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.64      0.03      0.06       231\n","\n","    accuracy                           0.30       356\n","   macro avg       0.32      0.28      0.17       356\n","weighted avg       0.52      0.30      0.20       356\n","\n","The average F1-score for total test dataset is  0.23076923076923075\n"]}]},{"cell_type":"code","source":["## 0.6542 glove target pos_tag sentimet ngram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, solver='liblinear'),\n","    'Climate Change is a Real Concern': GradientBoostingClassifier(),\n","    'Feminist Movement':  RandomForestClassifier(max_depth=6),\n","    'Hillary Clinton':  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","    'Legalization of Abortion': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","}\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tlg4fvVQM5nU","executionInfo":{"status":"ok","timestamp":1701064096540,"user_tz":300,"elapsed":105760,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"7e589440-b529-45f5-9fde-37988126d86c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.83      0.75      0.79       535\n","       FAVOR       0.68      0.55      0.61       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.68       824\n","   macro avg       0.50      0.43      0.47       824\n","weighted avg       0.78      0.68      0.73       824\n","\n","The average F1-score for total test dataset is  0.6986715314809618\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.61      0.58      0.59       173\n","       FAVOR       0.15      0.60      0.23        15\n","        NONE       0.70      0.56      0.62       194\n","\n","    accuracy                           0.57       382\n","   macro avg       0.48      0.58      0.48       382\n","weighted avg       0.64      0.57      0.59       382\n","\n","The average F1-score for total test dataset is  0.41361902192762734\n","The average F1-score for task B is  0.3586214539007093\n","The best Classifier for task B is  RandomForestClassifier(max_depth=6)\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.43      0.95      0.59       299\n","       FAVOR       0.30      0.08      0.13       148\n","        NONE       0.83      0.02      0.04       260\n","\n","    accuracy                           0.42       707\n","   macro avg       0.52      0.35      0.25       707\n","weighted avg       0.55      0.42      0.29       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.55      0.96      0.70       176\n","       FAVOR       0.63      0.08      0.15       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.56       324\n","   macro avg       0.40      0.35      0.28       324\n","weighted avg       0.58      0.56      0.45       324\n","\n","The average F1-score for total test dataset is  0.4245220830586684\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.34      0.93      0.50       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.75      0.01      0.03       231\n","\n","    accuracy                           0.33       356\n","   macro avg       0.36      0.31      0.17       356\n","weighted avg       0.60      0.33      0.19       356\n","\n","The average F1-score for total test dataset is  0.24835164835164836\n"]}]},{"cell_type":"code","source":["## tfidf ngram target\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': GradientBoostingClassifier(n_estimators=25),\n","    'Climate Change is a Real Concern': SVC(gamma=0.1, kernel='linear', probability = True),\n","    'Feminist Movement': GradientBoostingClassifier(max_depth=5, n_estimators=25),\n","    'Hillary Clinton': GradientBoostingClassifier(max_depth=4),\n","    'Legalization of Abortion': RandomForestClassifier(max_depth=7, n_estimators=50)\n","}\n","\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhldcA9d26-R","executionInfo":{"status":"ok","timestamp":1701140568495,"user_tz":300,"elapsed":17259,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"8e093b7c-2b7d-4145-e585-92e3244e96f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.82      0.78      0.80       535\n","       FAVOR       0.69      0.50      0.58       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.68       824\n","   macro avg       0.50      0.43      0.46       824\n","weighted avg       0.77      0.68      0.72       824\n","\n","The average F1-score for total test dataset is  0.6893441725253\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.55      0.66      0.60       173\n","       FAVOR       0.15      0.40      0.22        15\n","        NONE       0.68      0.46      0.55       194\n","\n","    accuracy                           0.55       382\n","   macro avg       0.46      0.51      0.46       382\n","weighted avg       0.60      0.55      0.56       382\n","\n","The average F1-score for total test dataset is  0.41013802950975725\n","The average F1-score for task B is  0.4219521125863661\n","The best Classifier for task B is  GradientBoostingClassifier(max_depth=5, n_estimators=25)\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.45      0.60      0.51       299\n","       FAVOR       0.28      0.41      0.33       148\n","        NONE       0.43      0.16      0.24       260\n","\n","    accuracy                           0.40       707\n","   macro avg       0.39      0.39      0.36       707\n","weighted avg       0.41      0.40      0.37       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.61      0.70      0.65       176\n","       FAVOR       0.56      0.37      0.45       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.55       324\n","   macro avg       0.39      0.36      0.37       324\n","weighted avg       0.58      0.55      0.55       324\n","\n","The average F1-score for total test dataset is  0.5486046486166918\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.34      0.66      0.45       122\n","       FAVOR       0.01      0.33      0.02         3\n","        NONE       0.67      0.06      0.11       231\n","\n","    accuracy                           0.27       356\n","   macro avg       0.34      0.35      0.19       356\n","weighted avg       0.55      0.27      0.23       356\n","\n","The average F1-score for total test dataset is  0.2339609877100011\n"]}]},{"cell_type":"code","source":["## ngram\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","classifiers = {\n","    'Atheism': GradientBoostingClassifier(n_estimators=25),\n","    'Climate Change is a Real Concern': SVC(gamma=0.1, kernel='linear', probability = True),\n","    'Feminist Movement': GradientBoostingClassifier(max_depth=5, n_estimators=25),\n","    'Hillary Clinton': GradientBoostingClassifier(max_depth=4),\n","    'Legalization of Abortion': RandomForestClassifier(max_depth=7, n_estimators=50)\n","}\n","\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2RxAwg_3eYL","executionInfo":{"status":"ok","timestamp":1701140584410,"user_tz":300,"elapsed":15316,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"72dc3bfe-8276-4f73-e96a-8b05e32d264d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     AGAINST       0.84      0.77      0.80       535\n","       FAVOR       0.68      0.51      0.58       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.68       824\n","   macro avg       0.51      0.42      0.46       824\n","weighted avg       0.78      0.68      0.72       824\n","\n","The average F1-score for total test dataset is  0.6904531912710687\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.54      0.64      0.59       173\n","       FAVOR       0.15      0.40      0.21        15\n","        NONE       0.65      0.45      0.53       194\n","\n","    accuracy                           0.54       382\n","   macro avg       0.44      0.50      0.44       382\n","weighted avg       0.58      0.54      0.55       382\n","\n","The average F1-score for total test dataset is  0.4007936507936508\n","The average F1-score for task B is  0.4208620333446379\n","The best Classifier for task B is  GradientBoostingClassifier(max_depth=5, n_estimators=25)\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.45      0.60      0.51       299\n","       FAVOR       0.28      0.41      0.33       148\n","        NONE       0.48      0.16      0.24       260\n","\n","    accuracy                           0.40       707\n","   macro avg       0.40      0.39      0.36       707\n","weighted avg       0.42      0.40      0.37       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.60      0.70      0.65       176\n","       FAVOR       0.57      0.37      0.45       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.54       324\n","   macro avg       0.39      0.35      0.36       324\n","weighted avg       0.58      0.54      0.55       324\n","\n","The average F1-score for total test dataset is  0.5463732861565679\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.34      0.68      0.45       122\n","       FAVOR       0.01      0.33      0.02         3\n","        NONE       0.68      0.06      0.10       231\n","\n","    accuracy                           0.27       356\n","   macro avg       0.35      0.36      0.19       356\n","weighted avg       0.56      0.27      0.22       356\n","\n","The average F1-score for total test dataset is  0.23770653862448807\n"]}]},{"cell_type":"markdown","source":["## Try something new!\n","\n","Apply voting classfier training and selection for each target"],"metadata":{"id":"7dl35frmNfj-"}},{"cell_type":"code","source":["def voting_train(train, test, feature_union, classify_pipeline, classifiers, params_grid, num=3):\n","    trained_classifiers = {}\n","    trained_feature_extraction = {}\n","    trained_voting = {}\n","    for name in train[\"Target\"].unique():\n","      trained_classifiers[name] = []\n","      X_train, y_train, X_test, y_test = split_data(train, test, name)\n","      best_score = 0\n","      X_train_transformed = feature_union.fit_transform(X_train)\n","      X_test_transformed = feature_union.transform(X_test)\n","      trained_feature_extraction[name] = feature_union\n","      for classifier_name, model in classifiers.items():\n","          classify_pipeline.set_params(classifier=model)\n","          grid_search = RandomizedSearchCV(classify_pipeline, param_distributions=params_grid[classifier_name], cv=5, verbose=0, random_state=595, n_jobs=-1)\n","          start = time.time()\n","          grid_search.fit(X_train_transformed, y_train)\n","          end = time.time()\n","          # print('The best parameter for {} is {}.'.format(classifier_name, grid_search.best_params_))\n","          # print(\"Grid Search for Model {} needs {} seconds.\".format(classifier_name, end-start))\n","          # print(\"The score for {} is {:.2f}.\".format(classifier_name, grid_search.best_score_))\n","          best_model = grid_search.best_estimator_\n","          score = report_score(feature_union, best_model, X_test_transformed, y_test)\n","          print(trained_classifiers[name])\n","          if len(trained_classifiers[name]) < num:\n","            trained_classifiers[name].append((best_model, score))\n","            trained_classifiers[name] = sorted(trained_classifiers[name], key = lambda x:x[1], reverse=True)\n","            best_score = trained_classifiers[name][-1][1]\n","          elif score > best_score:\n","              trained_classifiers[name].pop()\n","              trained_classifiers[name].append((best_model, score))\n","              trained_classifiers[name] = sorted(trained_classifiers[name], key = lambda x:x[1], reverse=True)\n","              best_score = trained_classifiers[name][-1][1]\n","\n","      voting_classifier = VotingClassifier(estimators= [(f\"clf{i}\", trained_classifiers[name][i][0]) for i in range(len(trained_classifiers[name]))])\n","      voting_classifier.fit(X_train_transformed, y_train)\n","      voting_score = report_score(feature_union, voting_classifier, X_test_transformed, y_test)\n","      print(f\"Voting score for {name} is {voting_score}.\")\n","      trained_voting[name] = voting_classifier\n","    print(\"Finish training and voting model selection.\")\n","\n","    ## test part\n","    predictions = pd.DataFrame(index=test.index)\n","\n","    for target in trained_voting:\n","        # Select the test data for the current target\n","        target_data = test[test['Target'] == target][['Tweet', 'Target']]\n","\n","        clf = trained_voting[target]\n","        feature_union = trained_feature_extraction[target]\n","        transformed_features = feature_union.transform(target_data)\n","        target_predictions = clf.predict(transformed_features)\n","        predictions.loc[target_data.index, 'Prediction'] = target_predictions\n","\n","    print(classification_report(test['Stance'], predictions))\n","    report = classification_report(test['Stance'], predictions, output_dict=True, zero_division=0)\n","    f1_favor = report['FAVOR']['f1-score']\n","    f1_against = report['AGAINST']['f1-score']\n","    score = (f1_favor + f1_against)/2\n","    print(\"The average F1-score for total test dataset is \", score)\n","\n","    return score, predictions\n"],"metadata":{"id":"ap-Xfun-IU9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trained_classifiers = {}\n","trained_feature_extraction = {}\n","trained_voting = {}\n","for name in train[\"Target\"].unique():\n","  trained_classifiers[name] = []\n","  X_train, y_train, X_test, y_test = split_data(train, test, name)\n","  best_score = 0\n","  X_train_transformed = feature_union.fit_transform(X_train)\n","  X_test_transformed = feature_union.transform(X_test)\n","  trained_feature_extraction[name] = feature_union\n","  for classifier_name, model in classifiers.items():\n","      classify_pipeline.set_params(classifier=model)\n","      grid_search = RandomizedSearchCV(classify_pipeline, param_distributions=params_grid[classifier_name], cv=5, verbose=0, random_state=595, n_jobs=-1)\n","      start = time.time()\n","      grid_search.fit(X_train_transformed, y_train)\n","      end = time.time()\n","      # print('The best parameter for {} is {}.'.format(classifier_name, grid_search.best_params_))\n","      # print(\"Grid Search for Model {} needs {} seconds.\".format(classifier_name, end-start))\n","      # print(\"The score for {} is {:.2f}.\".format(classifier_name, grid_search.best_score_))\n","      best_model = grid_search.best_estimator_\n","      score = report_score(feature_union, best_model, X_test_transformed, y_test)\n","      print(trained_classifiers[name])\n","      if len(trained_classifiers[name]) < 3:\n","        trained_classifiers[name].append((best_model, score))\n","        trained_classifiers[name] = sorted(trained_classifiers[name], key = lambda x:x[1], reverse=True)\n","        best_score = trained_classifiers[name][-1][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"oMzoe6rvUxTE","executionInfo":{"status":"error","timestamp":1702088321873,"user_tz":300,"elapsed":101799,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"c9f8700e-9e36-40e3-e7d9-37a0b616ce24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-1e735c1c6e66>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrained_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"]}]},{"cell_type":"code","source":["trained_classifiers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5khR1jWU001","executionInfo":{"status":"ok","timestamp":1702088790851,"user_tz":300,"elapsed":155,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"43fae271-4114-45d0-edcd-03c84d93d546"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Atheism']"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["## glove pos-tag bigram on voting classifier\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","transformers.append(('pos_tag', Pipeline([\n","    ('pos_extractor', PosTagVectorizer()),\n","    ('vectorizer', DictVectorizer())\n","])))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","bigram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('bigram', bigram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","\n","voting_train(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","# transformers = []\n","# transformers.append(('glove', glove_vectorizer))\n","# # word2vec_model = train_word2vec(train)\n","# # word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# # transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# # transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# # transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","# trump_feature_union = FeatureUnion(transformers)\n","\n","# test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","# test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","# predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","# test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","# test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PsVbRpwg-yt5","executionInfo":{"status":"ok","timestamp":1702092112598,"user_tz":300,"elapsed":2989406,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"cd8995e1-ad6d-4670-90d0-22934559bd06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=0.1, solver='newton-cg'))]), 0.5377415458937198)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=0.1, solver='newton-cg'))]), 0.5377415458937198), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=6))]), 0.4657937601804285)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=0.1, solver='newton-cg'))]), 0.5377415458937198), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=6))]), 0.4657937601804285), (Pipeline(steps=[('classifier', RandomForestClassifier(max_depth=6))]), 0.44474153297682717)]\n","[(Pipeline(steps=[('classifier', SVC(class_weight='balanced', kernel='linear'))]), 0.6575235109717869), (Pipeline(steps=[('classifier', LogisticRegression(C=0.1, solver='newton-cg'))]), 0.5377415458937198), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=6))]), 0.4657937601804285)]\n","Voting score for Atheism is 0.624929178470255.\n","[]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.456777316735823)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.456777316735823), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=7))]), 0.35135135135135137)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.456777316735823), (Pipeline(steps=[('classifier',\n","                 RandomForestClassifier(max_depth=6, n_estimators=50))]), 0.39607843137254894), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=7))]), 0.35135135135135137)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.456777316735823), (Pipeline(steps=[('classifier',\n","                 RandomForestClassifier(max_depth=6, n_estimators=50))]), 0.39607843137254894), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=7))]), 0.35135135135135137)]\n","Voting score for Climate Change is a Real Concern is 0.47246168582375475.\n","[]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.47053872053872053)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.47053872053872053), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=7))]), 0.4656848862456339)]\n","[(Pipeline(steps=[('classifier',\n","                 RandomForestClassifier(max_depth=7, n_estimators=50))]), 0.5288406772640191), (Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.47053872053872053), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=7))]), 0.4656848862456339)]\n","[(Pipeline(steps=[('classifier',\n","                 RandomForestClassifier(max_depth=7, n_estimators=50))]), 0.5288406772640191), (Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.47053872053872053), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=7))]), 0.4656848862456339)]\n","Voting score for Feminist Movement is 0.5004847897224579.\n","[]\n","[(Pipeline(steps=[('classifier',\n","                 LogisticRegression(C=0.01, class_weight='balanced',\n","                                    fit_intercept=False, solver='liblinear'))]), 0.47228703499889946)]\n","[(Pipeline(steps=[('classifier',\n","                 LogisticRegression(C=0.01, class_weight='balanced',\n","                                    fit_intercept=False, solver='liblinear'))]), 0.47228703499889946), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=6))]), 0.4169007204786909)]\n","[(Pipeline(steps=[('classifier',\n","                 LogisticRegression(C=0.01, class_weight='balanced',\n","                                    fit_intercept=False, solver='liblinear'))]), 0.47228703499889946), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=6))]), 0.4169007204786909), (Pipeline(steps=[('classifier',\n","                 RandomForestClassifier(criterion='entropy', max_depth=5,\n","                                        n_estimators=25))]), 0.3681917211328976)]\n","[(Pipeline(steps=[('classifier',\n","                 LogisticRegression(C=0.01, class_weight='balanced',\n","                                    fit_intercept=False, solver='liblinear'))]), 0.47228703499889946), (Pipeline(steps=[('classifier', KNeighborsClassifier(n_neighbors=6))]), 0.4169007204786909), (Pipeline(steps=[('classifier', SVC(gamma=10))]), 0.3683083511777302)]\n","Voting score for Hillary Clinton is 0.4138127853881279.\n","[]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.5080034423407918)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.5080034423407918), (Pipeline(steps=[('classifier', KNeighborsClassifier())]), 0.48371084136320036)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.5080034423407918), (Pipeline(steps=[('classifier', KNeighborsClassifier())]), 0.48371084136320036), (Pipeline(steps=[('classifier', RandomForestClassifier(max_depth=6))]), 0.39955357142857134)]\n","[(Pipeline(steps=[('classifier', LogisticRegression(C=1))]), 0.5080034423407918), (Pipeline(steps=[('classifier', KNeighborsClassifier())]), 0.48371084136320036), (Pipeline(steps=[('classifier', SVC(gamma=0.1, kernel='linear'))]), 0.4413553844691569)]\n","Voting score for Legalization of Abortion is 0.48566176470588235.\n","Finish training and voting model selection.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.68      0.84      0.75       715\n","       FAVOR       0.72      0.45      0.55       304\n","        NONE       0.34      0.27      0.31       230\n","\n","    accuracy                           0.64      1249\n","   macro avg       0.58      0.52      0.54      1249\n","weighted avg       0.63      0.64      0.62      1249\n","\n","The average F1-score for total test dataset is  0.6536983448930536\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.6536983448930536,\n","      Prediction\n"," 0       AGAINST\n"," 1       AGAINST\n"," 2       AGAINST\n"," 3          NONE\n"," 4       AGAINST\n"," ...         ...\n"," 1244       NONE\n"," 1245       NONE\n"," 1246       NONE\n"," 1247       NONE\n"," 1248    AGAINST\n"," \n"," [1249 rows x 1 columns])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["## only. sentiment\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","transformers.append(('sentiment', SentimentExtractor()))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","\n","# classifiers = {\n","#     'Atheism': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, solver='liblinear'),\n","#     'Climate Change is a Real Concern': GradientBoostingClassifier(),\n","#     'Feminist Movement':  RandomForestClassifier(max_depth=6),\n","#     'Hillary Clinton':  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","#     'Legalization of Abortion': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","# }\n","\n","# transformers = []\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# trump['Tweet'] = transform_all(trump['Tweet'])\n","# X_train = trump[['Tweet', 'Target']]\n","# y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","# ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","# ngram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('ngram', ngram_transformer))\n","\n","# trump_feature_union = FeatureUnion(transformers)\n","\n","# test_on_opinionA(train, opinion_to_target_A, feature_union, classifiers)\n","# test_on_opinionA(train, opinion_to_other_A, feature_union, classifiers)\n","# predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, classifiers)\n","# test_on_opinionB(train, opinion_to_target_B, trump_feature_union, classifiers, best_f1_targetmodel)\n","# test_on_opinionB(train, opinion_to_other_B, trump_feature_union, classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1n6K1SAVp7h","executionInfo":{"status":"ok","timestamp":1702137119808,"user_tz":300,"elapsed":150373,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"8c0c5705-d02e-46c5-d7d9-a17b3d0ec00d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The best model for Atheism is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=4, n_estimators=10), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 4, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is RandomForest with the average of F1 as 0.4487841945288754.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear'))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear'), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': False, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.40073529411764713.\n","The best model for Feminist Movement is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=6))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=6), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Feminist Movement is KNN with the average of F1 as 0.453211275791921.\n","The best model for Hillary Clinton is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(criterion='entropy', max_depth=3, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(criterion='entropy', max_depth=3, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'entropy', 'classifier__max_depth': 3, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is RandomForest with the average of F1 as 0.4659550380074397.\n","The best model for Legalization of Abortion is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(criterion='entropy', max_depth=3, n_estimators=50))], 'verbose': False, 'classifier': RandomForestClassifier(criterion='entropy', max_depth=3, n_estimators=50), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'entropy', 'classifier__max_depth': 3, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is RandomForest with the average of F1 as 0.40570175438596495.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.67      0.86      0.75       715\n","       FAVOR       0.50      0.47      0.49       304\n","        NONE       0.24      0.04      0.07       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.47      0.46      0.44      1249\n","weighted avg       0.55      0.61      0.56      1249\n","\n","The average F1-score for total test dataset is  0.6179860510525697\n"]}]},{"cell_type":"code","source":["## only. sentiment\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","\n","# classifiers = {\n","#     'Atheism': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, solver='liblinear'),\n","#     'Climate Change is a Real Concern': GradientBoostingClassifier(),\n","#     'Feminist Movement':  RandomForestClassifier(max_depth=6),\n","#     'Hillary Clinton':  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","#     'Legalization of Abortion': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","# }\n","\n","transformers = []\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, trained_classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, trained_classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, trained_classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, trained_classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, trained_classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_OkzHTz-HGF","executionInfo":{"status":"ok","timestamp":1702350395866,"user_tz":300,"elapsed":210179,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"a3344f5f-fa04-463d-960f-e3a2ae96d964"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=4, n_estimators=10))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=4, n_estimators=10), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 4, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 10, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6963166144200628.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'), 'classifier__C': 0.1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.43447874199184633.\n","The best model for Feminist Movement is RandomForest with {'memory': None, 'steps': [('classifier', RandomForestClassifier(max_depth=6))], 'verbose': False, 'classifier': RandomForestClassifier(max_depth=6), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 100, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is RandomForest with the average of F1 as 0.514634272689598.\n","The best model for Hillary Clinton is KNN with {'memory': None, 'steps': [('classifier', KNeighborsClassifier(n_neighbors=7))], 'verbose': False, 'classifier': KNeighborsClassifier(n_neighbors=7), 'classifier__algorithm': 'auto', 'classifier__leaf_size': 30, 'classifier__metric': 'minkowski', 'classifier__metric_params': None, 'classifier__n_jobs': None, 'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n","The best classifier for Hillary Clinton is KNN with the average of F1 as 0.39273289524545807.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6071979089873333.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.74      0.73      0.73       715\n","       FAVOR       0.57      0.41      0.48       304\n","        NONE       0.38      0.53      0.44       230\n","\n","    accuracy                           0.61      1249\n","   macro avg       0.56      0.56      0.55      1249\n","weighted avg       0.63      0.61      0.62      1249\n","\n","The average F1-score for total test dataset is  0.6067136812411847\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.82      0.75      0.79       535\n","       FAVOR       0.64      0.41      0.50       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.63       824\n","   macro avg       0.49      0.39      0.43       824\n","weighted avg       0.76      0.63      0.69       824\n","\n","The average F1-score for total test dataset is  0.6435546875\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.57      0.64      0.60       173\n","       FAVOR       0.20      0.47      0.28        15\n","        NONE       0.67      0.53      0.59       194\n","\n","    accuracy                           0.58       382\n","   macro avg       0.48      0.54      0.49       382\n","weighted avg       0.61      0.58      0.58       382\n","\n","The average F1-score for total test dataset is  0.44163043478260866\n","The average F1-score for task B is  0.4964375925442034\n","The best Classifier for task B is  Pipeline(steps=[('classifier', RandomForestClassifier(max_depth=6))])\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.44      0.87      0.58       299\n","       FAVOR       0.48      0.36      0.41       148\n","        NONE       1.00      0.02      0.05       260\n","\n","    accuracy                           0.45       707\n","   macro avg       0.64      0.42      0.35       707\n","weighted avg       0.65      0.45      0.35       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.61      0.86      0.72       176\n","       FAVOR       0.68      0.36      0.47       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.63       324\n","   macro avg       0.43      0.41      0.40       324\n","weighted avg       0.64      0.63      0.60       324\n","\n","The average F1-score for total test dataset is  0.5937846836847946\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.33      0.87      0.48       122\n","       FAVOR       0.03      0.33      0.06         3\n","        NONE       1.00      0.03      0.05       231\n","\n","    accuracy                           0.32       356\n","   macro avg       0.45      0.41      0.20       356\n","weighted avg       0.76      0.32      0.20       356\n","\n","The average F1-score for total test dataset is  0.26923563654770943\n"]}]},{"cell_type":"code","source":["## glove ngram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","transformers.append(('glove', glove_vectorizer))\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","\n","# classifiers = {\n","#     'Atheism': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, solver='liblinear'),\n","#     'Climate Change is a Real Concern': GradientBoostingClassifier(),\n","#     'Feminist Movement':  RandomForestClassifier(max_depth=6),\n","#     'Hillary Clinton':  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","#     'Legalization of Abortion': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","# }\n","\n","transformers = []\n","transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","# transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, trained_classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, trained_classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, trained_classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, trained_classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, trained_classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RioIVtW1Ayez","executionInfo":{"status":"ok","timestamp":1702359859360,"user_tz":300,"elapsed":2878133,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"dc4c746a-f94d-4c41-e52f-4de39680a671"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["The best model for Atheism is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'), 'classifier__C': 0.1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is LogisticRegression with the average of F1 as 0.6528862348798039.\n","The best model for Climate Change is a Real Concern is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'), 'classifier__C': 0.1, 'classifier__class_weight': 'balanced', 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'liblinear', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Climate Change is a Real Concern is LogisticRegression with the average of F1 as 0.5209985315712188.\n","The best model for Feminist Movement is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.01, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.01, solver='newton-cg'), 'classifier__C': 0.01, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is LogisticRegression with the average of F1 as 0.5282653588022715.\n","The best model for Hillary Clinton is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear', probability=True))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear', probability=True), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': True, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Hillary Clinton is SVM with the average of F1 as 0.45912568306010937.\n","The best model for Legalization of Abortion is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=0.1, solver='newton-cg'))], 'verbose': False, 'classifier': LogisticRegression(C=0.1, solver='newton-cg'), 'classifier__C': 0.1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'newton-cg', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Legalization of Abortion is LogisticRegression with the average of F1 as 0.6332937383336161.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.77      0.70      0.73       715\n","       FAVOR       0.55      0.53      0.54       304\n","        NONE       0.46      0.62      0.52       230\n","\n","    accuracy                           0.64      1249\n","   macro avg       0.59      0.62      0.60      1249\n","weighted avg       0.66      0.64      0.65      1249\n","\n","The average F1-score for total test dataset is  0.637066864861467\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.84      0.73      0.78       535\n","       FAVOR       0.66      0.53      0.59       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.66       824\n","   macro avg       0.50      0.42      0.46       824\n","weighted avg       0.77      0.66      0.71       824\n","\n","The average F1-score for total test dataset is  0.6849379058903953\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.62      0.57      0.59       173\n","       FAVOR       0.15      0.53      0.23        15\n","        NONE       0.70      0.61      0.65       194\n","\n","    accuracy                           0.59       382\n","   macro avg       0.49      0.57      0.49       382\n","weighted avg       0.64      0.59      0.61       382\n","\n","The average F1-score for total test dataset is  0.4132393262828046\n","The average F1-score for task B is  0.3841518773600726\n","The best Classifier for task B is  Pipeline(steps=[('classifier', LogisticRegression(C=0.01, solver='newton-cg'))])\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.42      0.96      0.59       299\n","       FAVOR       0.55      0.11      0.18       148\n","        NONE       0.00      0.00      0.00       260\n","\n","    accuracy                           0.43       707\n","   macro avg       0.33      0.36      0.26       707\n","weighted avg       0.29      0.43      0.29       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.57      0.98      0.72       176\n","       FAVOR       0.84      0.11      0.20       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.58       324\n","   macro avg       0.47      0.36      0.30       324\n","weighted avg       0.68      0.58      0.48       324\n","\n","The average F1-score for total test dataset is  0.4572283352771158\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.33      0.93      0.48       122\n","       FAVOR       0.00      0.00      0.00         3\n","        NONE       0.00      0.00      0.00       231\n","\n","    accuracy                           0.32       356\n","   macro avg       0.11      0.31      0.16       356\n","weighted avg       0.11      0.32      0.17       356\n","\n","The average F1-score for total test dataset is  0.24145299145299146\n"]}]},{"cell_type":"code","source":["## glove ngram\n","transformers = []\n","# glove_vectorizer = GloVeVectorizer(glove_path=args.glove_path, vector_size=200)\n","# transformers.append(('glove', glove_vectorizer))\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","train['Tweet'] = transform_all(train['Tweet'])\n","X_train = train[['Tweet', 'Target']]\n","y_train = train['Stance']\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","feature_union = FeatureUnion(transformers)\n","trained_feature_extraction, trained_classifiers = train_test_all(train, test, feature_union, classify_pipeline, classifiers, params_grid)\n","score, predictinos = test_all(test, trained_feature_extraction, trained_classifiers)\n","\n","# classifiers = {\n","#     'Atheism': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, solver='liblinear'),\n","#     'Climate Change is a Real Concern': GradientBoostingClassifier(),\n","#     'Feminist Movement':  RandomForestClassifier(max_depth=6),\n","#     'Hillary Clinton':  LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg'),\n","#     'Legalization of Abortion': LogisticRegression(C=0.1, max_iter=1000, solver='newton-cg')\n","# }\n","\n","transformers = []\n","transformers.append(('tfidf', ModifiedTfidfVectorizer(max_feature=500)))\n","# transformers.append(('glove', glove_vectorizer))\n","# word2vec_model = train_word2vec(train)\n","# word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n","# transformers.append(('word2vec', word2vec_vectorizer))\n","\n","# transformers.append(('pos_tag', Pipeline([\n","#     ('pos_extractor', PosTagVectorizer()),\n","#     ('vectorizer', DictVectorizer())\n","# ])))\n","transformers.append(('target_presence', TargetPresence(target_words)))\n","\n","# transformers.append(('sentiment', SentimentExtractor()))\n","\n","trump['Tweet'] = transform_all(trump['Tweet'])\n","X_train = trump[['Tweet', 'Target']]\n","y_train = trump['Stance']\n","\n","# bigram_transformer = NGramVectorizer(ngram_range_word=(2,2))\n","# bigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('bigram', bigram_transformer))\n","# trigram_transformer = NGramVectorizer(ngram_range_word=(3,3))\n","# trigram_transformer.fit_selector(X_train, y_train)\n","# transformers.append(('trigram', trigram_transformer))\n","\n","ngram_transformer = NGramVectorizer(ngram_range_word=(1,3), ngram_range_char=(2, 5))\n","ngram_transformer.fit_selector(X_train, y_train)\n","transformers.append(('ngram', ngram_transformer))\n","\n","trump_feature_union = FeatureUnion(transformers)\n","\n","test_on_opinionA(train, opinion_to_target_A, feature_union, trained_classifiers)\n","test_on_opinionA(train, opinion_to_other_A, feature_union, trained_classifiers)\n","predictions, best_f1_targetmodel = predict_model_taskB(train, trump, trump_feature_union, trained_classifiers)\n","test_on_opinionB(train, opinion_to_target_B, trump_feature_union, trained_classifiers, best_f1_targetmodel)\n","test_on_opinionB(train, opinion_to_other_B, trump_feature_union, trained_classifiers, best_f1_targetmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhMLBNSJA6w9","executionInfo":{"status":"ok","timestamp":1702360196694,"user_tz":300,"elapsed":337359,"user":{"displayName":"Eric shangguan","userId":"07562676671474168964"}},"outputId":"71fbcd21-2905-4c17-dbfe-95fbdf52b314"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["The best model for Atheism is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(n_estimators=25))], 'verbose': False, 'classifier': GradientBoostingClassifier(n_estimators=25), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 25, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Atheism is GradientBoosting with the average of F1 as 0.6647058823529413.\n","The best model for Climate Change is a Real Concern is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear', probability=True))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear', probability=True), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': True, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Climate Change is a Real Concern is SVM with the average of F1 as 0.37288135593220345.\n","The best model for Feminist Movement is GradientBoosting with {'memory': None, 'steps': [('classifier', GradientBoostingClassifier(max_depth=2, n_estimators=50))], 'verbose': False, 'classifier': GradientBoostingClassifier(max_depth=2, n_estimators=50), 'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'friedman_mse', 'classifier__init': None, 'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 2, 'classifier__max_features': None, 'classifier__max_leaf_nodes': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__n_estimators': 50, 'classifier__n_iter_no_change': None, 'classifier__random_state': None, 'classifier__subsample': 1.0, 'classifier__tol': 0.0001, 'classifier__validation_fraction': 0.1, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Feminist Movement is GradientBoosting with the average of F1 as 0.5316390707226286.\n","The best model for Hillary Clinton is LogisticRegression with {'memory': None, 'steps': [('classifier', LogisticRegression(C=1))], 'verbose': False, 'classifier': LogisticRegression(C=1), 'classifier__C': 1, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}\n","The best classifier for Hillary Clinton is LogisticRegression with the average of F1 as 0.4433845061673618.\n","The best model for Legalization of Abortion is SVM with {'memory': None, 'steps': [('classifier', SVC(gamma=0.1, kernel='linear', probability=True))], 'verbose': False, 'classifier': SVC(gamma=0.1, kernel='linear', probability=True), 'classifier__C': 1.0, 'classifier__break_ties': False, 'classifier__cache_size': 200, 'classifier__class_weight': None, 'classifier__coef0': 0.0, 'classifier__decision_function_shape': 'ovr', 'classifier__degree': 3, 'classifier__gamma': 0.1, 'classifier__kernel': 'linear', 'classifier__max_iter': -1, 'classifier__probability': True, 'classifier__random_state': None, 'classifier__shrinking': True, 'classifier__tol': 0.001, 'classifier__verbose': False}\n","The best classifier for Legalization of Abortion is SVM with the average of F1 as 0.6244941364133164.\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.72      0.67      0.69       715\n","       FAVOR       0.53      0.47      0.50       304\n","        NONE       0.35      0.47      0.40       230\n","\n","    accuracy                           0.59      1249\n","   macro avg       0.53      0.54      0.53      1249\n","weighted avg       0.60      0.59      0.59      1249\n","\n","The average F1-score for total test dataset is  0.5968501086169442\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.85      0.75      0.79       535\n","       FAVOR       0.66      0.55      0.60       289\n","        NONE       0.00      0.00      0.00         0\n","\n","    accuracy                           0.68       824\n","   macro avg       0.50      0.43      0.46       824\n","weighted avg       0.78      0.68      0.73       824\n","\n","The average F1-score for total test dataset is  0.6971873860804925\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.55      0.60      0.57       173\n","       FAVOR       0.13      0.40      0.19        15\n","        NONE       0.67      0.51      0.58       194\n","\n","    accuracy                           0.54       382\n","   macro avg       0.45      0.50      0.45       382\n","weighted avg       0.60      0.54      0.56       382\n","\n","The average F1-score for total test dataset is  0.38406701122794507\n","The average F1-score for task B is  0.43091296121097444\n","The best Classifier for task B is  Pipeline(steps=[('classifier',\n","                 GradientBoostingClassifier(max_depth=2, n_estimators=50))])\n","The classification reporst is:                precision    recall  f1-score   support\n","\n","     AGAINST       0.45      0.68      0.54       299\n","       FAVOR       0.29      0.36      0.32       148\n","        NONE       0.49      0.12      0.19       260\n","\n","    accuracy                           0.41       707\n","   macro avg       0.41      0.39      0.35       707\n","weighted avg       0.43      0.41      0.37       707\n","\n","The most similiar target for task B is  Feminist Movement\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.57      0.74      0.64       176\n","       FAVOR       0.53      0.26      0.34       145\n","        NONE       0.00      0.00      0.00         3\n","\n","    accuracy                           0.52       324\n","   macro avg       0.37      0.33      0.33       324\n","weighted avg       0.54      0.52      0.50       324\n","\n","The average F1-score for total test dataset is  0.4939603451231358\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.35      0.70      0.47       122\n","       FAVOR       0.01      0.33      0.02         3\n","        NONE       0.73      0.05      0.09       231\n","\n","    accuracy                           0.28       356\n","   macro avg       0.36      0.36      0.19       356\n","weighted avg       0.60      0.28      0.22       356\n","\n","The average F1-score for total test dataset is  0.24337160897382168\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}